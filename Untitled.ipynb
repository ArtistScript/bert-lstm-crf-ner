{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from bert_base.train import tf_metrics\n",
    "from bert_base.bert import modeling\n",
    "from bert_base.bert import optimization\n",
    "from bert_base.bert import tokenization\n",
    "\n",
    "# import\n",
    "\n",
    "from bert_base.train.models import create_model, InputFeatures, InputExample\n",
    "\n",
    "__version__ = '0.1.0'\n",
    "\n",
    "__all__ = ['__version__', 'DataProcessor', 'NerProcessor', 'write_tokens', 'convert_single_example',\n",
    "           'filed_based_convert_examples_to_features', 'file_based_input_fn_builder',\n",
    "           'model_fn_builder', 'train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0b8e0dee2fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from train.train_helper import get_args_parser\n",
    "from train.bert_lstm_ner import train\n",
    "args = get_args_parser()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "args.task_name=\"NER\"\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "args.do_predict=True\n",
    "args.data_dir=\"D:/project/python_project/bert-lstm-crf-ner\\data_demo\"\n",
    "args.vocab_file=\"D:/project/python_project/bert-lstm-crf-ner/bert\\chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "args.bert_config_file=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "args.init_checkpoint=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "args.max_seq_length=128\n",
    "args.train_batch_size=32\n",
    "args.learning_rate=2e-5\n",
    "args.num_train_epochs=3.0\n",
    "args.output_dir=\"D:/project/python_project/bert-lstm-crf-ner/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NerProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fcf6ddd02de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#一个处理的类，包括训练数据的输入等\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m processors = {\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNerProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     }\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#载入bert配置文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NerProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "    #一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "        \"ner\": NerProcessor\n",
    "    }\n",
    "    #载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-92c3eac34cb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mNerProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataProcessor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[1])\n",
    "                else:\n",
    "                    if len(contends) == 0:\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append([l, w])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    words.append('')\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-8-9be141168aa6>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-9be141168aa6>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    processors = {\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "    #一个处理的类，包括训练数据的输入等\n",
    "    processors = {\n",
    "        \"ner\": NerProcessor\n",
    "    }\n",
    "    #载入bert配置文件\n",
    "    bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "    #检查序列的最大长度是否超出范围\n",
    "    if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "        raise ValueError(\n",
    "            \"Cannot use sequence length %d because the BERT model \"\n",
    "            \"was only trained up to sequence length %d\" %\n",
    "            (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "    # 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "    if args.clean and args.do_train:\n",
    "        if os.path.exists(args.output_dir):\n",
    "            def del_file(path):\n",
    "                ls = os.listdir(path)\n",
    "                for i in ls:\n",
    "                    c_path = os.path.join(path, i)\n",
    "                    if os.path.isdir(c_path):\n",
    "                        del_file(c_path)\n",
    "                    else:\n",
    "                        os.remove(c_path)\n",
    "\n",
    "            try:\n",
    "                del_file(args.output_dir)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('pleace remove the files of output dir and data.conf')\n",
    "                exit(-1)\n",
    "\n",
    "    #check output dir exists\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "\n",
    "    #通过output_dir初始化数据处理类，processor\n",
    "    processor = processors[args.ner](args.output_dir)\n",
    "\n",
    "    #通过bert字典，初始化bert自带分词类\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    #创建session的时候，对session进行配置\n",
    "    session_config = tf.ConfigProto(\n",
    "        log_device_placement=False,#记录各项操作在哪台机器运行\n",
    "        inter_op_parallelism_threads=0,\n",
    "        intra_op_parallelism_threads=0,\n",
    "        allow_soft_placement=True)\n",
    "\n",
    "    #estimator运行配置，包括模型保存等\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        model_dir=args.output_dir,\n",
    "        save_summary_steps=500,\n",
    "        save_checkpoints_steps=500,\n",
    "        session_config=session_config\n",
    "    )\n",
    "\n",
    "    train_examples = None\n",
    "    eval_examples = None\n",
    "    num_train_steps = None\n",
    "    num_warmup_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "    #一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "        \"ner\": NerProcessor\n",
    "    }\n",
    "    #载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "    #检查序列的最大长度是否超出范围\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "# 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "if args.clean and args.do_train:\n",
    "    if os.path.exists(args.output_dir):\n",
    "        def del_file(path):\n",
    "            ls = os.listdir(path)\n",
    "            for i in ls:\n",
    "                c_path = os.path.join(path, i)\n",
    "                if os.path.isdir(c_path):\n",
    "                    del_file(c_path)\n",
    "                else:\n",
    "                    os.remove(c_path)\n",
    "\n",
    "        try:\n",
    "            del_file(args.output_dir)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pleace remove the files of output dir and data.conf')\n",
    "            exit(-1)\n",
    "\n",
    "#check output dir exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "#通过output_dir初始化数据处理类，processor\n",
    "processor = processors[args.ner](args.output_dir)\n",
    "\n",
    "#通过bert字典，初始化bert自带分词类\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#创建session的时候，对session进行配置\n",
    "session_config = tf.ConfigProto(\n",
    "    log_device_placement=False,#记录各项操作在哪台机器运行\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    allow_soft_placement=True)\n",
    "\n",
    "#estimator运行配置，包括模型保存等\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=args.output_dir,\n",
    "    save_summary_steps=500,\n",
    "    save_checkpoints_steps=500,\n",
    "    session_config=session_config\n",
    ")\n",
    "\n",
    "train_examples = None\n",
    "eval_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-10-ff6fc6f62aa7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-ff6fc6f62aa7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print args.data_dir\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "print args.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/project/python_project/bert-lstm-crf-ner\\data_demo\n"
     ]
    }
   ],
   "source": [
    "print(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<bert_base.train.models.InputExample object at 0x000001C32E4599B0>, <bert_base.train.models.InputExample object at 0x000001C32E459CC0>, <bert_base.train.models.InputExample object at 0x000001C32E459C18>, <bert_base.train.models.InputExample object at 0x000001C32E459A90>, <bert_base.train.models.InputExample object at 0x000001C32E4DB1D0>, <bert_base.train.models.InputExample object at 0x000001C32E4DB048>, <bert_base.train.models.InputExample object at 0x000001C32E4DBC50>, <bert_base.train.models.InputExample object at 0x000001C32E4DBDA0>, <bert_base.train.models.InputExample object at 0x000001C32E4DBEB8>, <bert_base.train.models.InputExample object at 0x000001C32E4DBD30>, <bert_base.train.models.InputExample object at 0x000001C32E4DBC18>, <bert_base.train.models.InputExample object at 0x000001C32E4DBC88>, <bert_base.train.models.InputExample object at 0x000001C32E4DBF98>, <bert_base.train.models.InputExample object at 0x000001C32E4DBCC0>, <bert_base.train.models.InputExample object at 0x000001C32E4DBD68>, <bert_base.train.models.InputExample object at 0x000001C32E4DB668>, <bert_base.train.models.InputExample object at 0x000001C32E4DBDD8>, <bert_base.train.models.InputExample object at 0x000001C32E4DB358>, <bert_base.train.models.InputExample object at 0x000001C32E4DBAC8>, <bert_base.train.models.InputExample object at 0x000001C32E4DB390>, <bert_base.train.models.InputExample object at 0x000001C32E4DB3C8>, <bert_base.train.models.InputExample object at 0x000001C32E4DBBA8>, <bert_base.train.models.InputExample object at 0x000001C32E4DB320>, <bert_base.train.models.InputExample object at 0x000001C32E4DB4A8>, <bert_base.train.models.InputExample object at 0x000001C32E4DB6A0>, <bert_base.train.models.InputExample object at 0x000001C32E4DB5C0>, <bert_base.train.models.InputExample object at 0x000001C32E4DBF60>, <bert_base.train.models.InputExample object at 0x000001C32E4DB438>, <bert_base.train.models.InputExample object at 0x000001C32E4DB550>, <bert_base.train.models.InputExample object at 0x000001C32E4DB710>, <bert_base.train.models.InputExample object at 0x000001C32E4DB080>, <bert_base.train.models.InputExample object at 0x000001C32E4DBA90>, <bert_base.train.models.InputExample object at 0x000001C32E4DB5F8>, <bert_base.train.models.InputExample object at 0x000001C32E4DB2B0>, <bert_base.train.models.InputExample object at 0x000001C32E4DBFD0>, <bert_base.train.models.InputExample object at 0x000001C32E4DB208>, <bert_base.train.models.InputExample object at 0x000001C32E4DB2E8>, <bert_base.train.models.InputExample object at 0x000001C32E4DB978>, <bert_base.train.models.InputExample object at 0x000001C32E4DBE48>, <bert_base.train.models.InputExample object at 0x000001C32E4DB400>, <bert_base.train.models.InputExample object at 0x000001C32E4DBB00>, <bert_base.train.models.InputExample object at 0x000001C32E4DB748>, <bert_base.train.models.InputExample object at 0x000001C32E4DB7B8>]\n"
     ]
    }
   ],
   "source": [
    "print(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
