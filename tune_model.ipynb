{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: c:\\python36\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\daizhutao\\AppData\\Roaming\\jupyter\\runtime\\kernel-daaf6e0b-c6df-4bb8-9a1f-6eca1276fdac.json\n",
      "                 ARG   VALUE\n",
      "__________________________________________________\n",
      "          batch_size = 64\n",
      "    bert_config_file = D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_config.json\n",
      "                cell = lstm\n",
      "               clean = True\n",
      "                clip = 0.5\n",
      "            data_dir = D:/project/python_project/bert-lstm-crf-ner/data\n",
      "          device_map = 0\n",
      "             do_eval = True\n",
      "       do_lower_case = True\n",
      "          do_predict = True\n",
      "            do_train = True\n",
      "        dropout_rate = 0.5\n",
      "     filter_adam_var = True\n",
      "     init_checkpoint = D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\n",
      "          label_list = None\n",
      "       learning_rate = 2e-05\n",
      "           lstm_size = 128\n",
      "      max_seq_length = 128\n",
      "                 ner = ner\n",
      "          num_layers = 1\n",
      "    num_train_epochs = 3.0\n",
      "          output_dir = D:/project/python_project/bert-lstm-crf-ner/output\n",
      "save_checkpoints_steps = 500\n",
      "  save_summary_steps = 500\n",
      "           task_name = NER\n",
      "    train_batch_size = 32\n",
      "             verbose = False\n",
      "          vocab_file = D:/project/python_project/bert-lstm-crf-ner/bert\\chinese_L-12_H-768_A-12/vocab.txt\n",
      "   warmup_proportion = 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#初始化配置\n",
    "import os\n",
    "import sys\n",
    "from train.train_helper import get_args_parser\n",
    "from train.bert_lstm_ner_cg_estimator import train\n",
    "\n",
    "args = get_args_parser()\n",
    "# print(args)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "args.task_name=\"NER\"\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "args.do_predict=True\n",
    "args.data_dir=\"D:/project/python_project/bert-lstm-crf-ner/data\"\n",
    "args.vocab_file=\"D:/project/python_project/bert-lstm-crf-ner/bert\\chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "args.bert_config_file=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "args.init_checkpoint=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "args.max_seq_length=128\n",
    "args.train_batch_size=32\n",
    "args.learning_rate=2e-5\n",
    "args.num_train_epochs=3.0\n",
    "args.output_dir=\"D:/project/python_project/bert-lstm-crf-ner/output\"\n",
    "#打印相关参数\n",
    "param_str = '\\n'.join(['%20s = %s' % (k, v) for k, v in sorted(vars(args).items())])\n",
    "print('usage: %s\\n%20s   %s\\n%s\\n%s\\n' % (' '.join(sys.argv), 'ARG', 'VALUE', '_' * 50, param_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "#sys.path.append('/home/idm/dzt/models/nlp_model/bert-lstm-crf-ner/')\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from train import tf_metrics\n",
    "from bert import modeling\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "# import\n",
    "\n",
    "from train.models import create_model, InputFeatures, InputExample\n",
    "\n",
    "__version__ = '0.1.0'\n",
    "\n",
    "__all__ = ['__version__', 'DataProcessor', 'NerProcessor', 'write_tokens', 'convert_single_example',\n",
    "           'filed_based_convert_examples_to_features', 'file_based_input_fn_builder',\n",
    "           'model_fn_builder', 'train']\n",
    "\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[1])\n",
    "                else:\n",
    "                    if len(contends) == 0:\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append([l, w])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    words.append('')\n",
    "                    continue\n",
    "            return lines\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        #比如set_type是train，就表示训练数据\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            #使用bert内部的tokenization包，把字符串转化成unicode\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            #模型训练的输入类，guid为唯一数据id\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines\n",
    "\n",
    "\n",
    "def write_tokens(tokens, output_dir, mode):\n",
    "    \"\"\"\n",
    "    将序列解析结果写入到文件中\n",
    "    只在mode=test的时候启用\n",
    "    :param tokens:\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if mode == \"test\":\n",
    "        path = os.path.join(output_dir, \"token_\" + mode + \".txt\")\n",
    "        wf = codecs.open(path, 'a', encoding='utf-8')\n",
    "        for token in tokens:\n",
    "            if token != \"**NULL**\":\n",
    "                wf.write(token + '\\n')\n",
    "        wf.close()\n",
    "\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode):\n",
    "    \"\"\"\n",
    "    将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
    "    :param ex_index: index\n",
    "    :param example: 一个样本\n",
    "    :param label_list: 标签列表\n",
    "    :param max_seq_length:\n",
    "    :param tokenizer:\n",
    "    :param output_dir\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label_map = {}\n",
    "    # 1表示从1开始对label进行index化\n",
    "    for (i, label) in enumerate(label_list, 1):\n",
    "        label_map[label] = i\n",
    "    # 保存label->index 的map\n",
    "    if not os.path.exists(os.path.join(output_dir, 'label2id.pkl')):\n",
    "        with codecs.open(os.path.join(output_dir, 'label2id.pkl'), 'wb') as w:\n",
    "            pickle.dump(label_map, w)\n",
    "\n",
    "    #因为从训练数据读取后，字，标签标记，都是用空格分隔的\n",
    "    textlist = example.text.split(' ')\n",
    "    labellist = example.label.split(' ')\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for i, word in enumerate(textlist):\n",
    "        # 分词，如果是中文，就是分字,但是对于一些不在BERT的vocab.txt中得字符会被进行WordPice处理（例如中文的引号），可以将所有的分字操作替换为list(input)\n",
    "        token = tokenizer.tokenize(word)\n",
    "        tokens.extend(token)\n",
    "        label_1 = labellist[i]\n",
    "        for m in range(len(token)):\n",
    "            if m == 0:\n",
    "                labels.append(label_1)\n",
    "            else:  # 一般不会出现else，因为只有一个词\n",
    "                labels.append(\"X\")\n",
    "    # tokens = tokenizer.tokenize(example.text)\n",
    "    # 序列截断\n",
    "    if len(tokens) >= max_seq_length - 1:\n",
    "        tokens = tokens[0:(max_seq_length - 2)]  # -2 的原因是因为序列需要加一个句首和句尾标志\n",
    "        labels = labels[0:(max_seq_length - 2)]\n",
    "    ntokens = []\n",
    "    segment_ids = []  #segment_ids的作用是？？？\n",
    "    label_ids = []\n",
    "    ntokens.append(\"[CLS]\")  # 句子开始设置CLS 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[CLS]\") not sure!\n",
    "    label_ids.append(label_map[\"[CLS]\"])  # O OR CLS 没有任何影响，不过我觉得O 会减少标签个数,不过句首和句尾使用不同的标志来标注，使用LCS 也没毛病\n",
    "    for i, token in enumerate(tokens):\n",
    "        ntokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(label_map[labels[i]])\n",
    "    ntokens.append(\"[SEP]\")  # 句尾添加[SEP] 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "    label_ids.append(label_map[\"[SEP]\"])\n",
    "    #调用bert内部的token2id函数，把token转化成bert使用的tokenid\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(ntokens)  # 将序列中的字(ntokens)转化为ID形式\n",
    "    input_mask = [1] * len(input_ids)   #mask是隐藏的token，用于模型训练\n",
    "    # label_mask = [1] * len(input_ids)\n",
    "    # padding, 使用\n",
    "    #小于序列长度的，进行补全操作\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        # we don't concerned about it!\n",
    "        label_ids.append(0)\n",
    "        ntokens.append(\"**NULL**\")\n",
    "        # label_mask.append(0)\n",
    "    # print(len(input_ids))\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "    # assert len(label_mask) == max_seq_length\n",
    "\n",
    "    # 打印部分样本数据信息\n",
    "    if ex_index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "            [tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "        # tf.logging.info(\"label_mask: %s\" % \" \".join([str(x) for x in label_mask]))\n",
    "\n",
    "    # 结构化为一个类，使用自定义类保存训练index化数据，\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_ids=label_ids,\n",
    "        # label_mask = label_mask\n",
    "    )\n",
    "    # mode='test'的时候才有效，把token写入文件\n",
    "    write_tokens(ntokens, output_dir, mode)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def filed_based_convert_examples_to_features(\n",
    "        examples, label_list, max_seq_length, tokenizer, output_file, output_dir, mode=None):\n",
    "    \"\"\"\n",
    "    将数据转化为TF_Record 结构，作为模型数据输入\n",
    "    :param examples:  样本\n",
    "    :param label_list:标签list\n",
    "    :param max_seq_length: 预先设定的最大序列长度\n",
    "    :param tokenizer: tokenizer 对象\n",
    "    :param output_file: tf.record 输出路径\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    # 遍历训练数据\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "        # 对于每一个训练样本,将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
    "        # 返回的是bert模型训练需要的index化token，label，mask，segment等信息\n",
    "        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()#除了记录k,v，还会记录k放入的顺序\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature(feature.label_ids)\n",
    "        # features[\"label_mask\"] = create_int_feature(feature.label_mask)\n",
    "        # tf.train.Example/Feature 是一种协议，方便序列化？？？\n",
    "        # Example中存放features特征，放入example是为了便于把特征序列化存储\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "\n",
    "def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder,batch_size):\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        # \"label_ids\":tf.VarLenFeature(tf.int64),\n",
    "        # \"label_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        #把recod变成字典example？,可能写入，读取都是按照tensorflow的某个标准\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "        d = d.repeat()\n",
    "        d = d.shuffle(buffer_size=300)\n",
    "    #通过map函数，调用_decode_record，把int64的数据转化成int32的数据，通过apply，把数据转化成batch的形式\n",
    "    d = d.apply(tf.data.experimental.map_and_batch(lambda record: _decode_record(record, name_to_features),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_parallel_calls=8,  # 并行处理数据的CPU核心数量，不要大于你机器的核心数\n",
    "                                                   drop_remainder=drop_remainder))\n",
    "    d = d.prefetch(buffer_size=4)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, args):\n",
    "    \"\"\"\n",
    "    构建模型\n",
    "    :param bert_config:\n",
    "    :param num_labels:\n",
    "    :param init_checkpoint:\n",
    "    :param learning_rate:\n",
    "    :param num_train_steps:\n",
    "    :param num_warmup_steps:\n",
    "    :param use_tpu:\n",
    "    :param use_one_hot_embeddings:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        print('shape of input_ids', input_ids.shape)\n",
    "        # label_mask = features[\"label_mask\"]\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        # 使用参数构建模型,input_idx 就是输入的样本idx表示，label_ids 就是标签的idx表示\n",
    "        #全部损失，分数，，预测类别\n",
    "        total_loss, logits, trans, pred_ids = create_model(\n",
    "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "            num_labels, False, args.dropout_rate, args.lstm_size, args.cell, args.num_layers)\n",
    "        # tf.summary.scalar('total_loss', total_loss)\n",
    "        # tf.summary.scalar('logits',logits)\n",
    "        # tf.summary.scalar('trans',trans)\n",
    "        # tf.summary.scalar('pred_ids',pred_ids)\n",
    "        #所有需要训练的变量\n",
    "        tvars = tf.trainable_variables()\n",
    "        # 加载BERT模型，assignmen_map，加载的预训练变量值\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names) = \\\n",
    "                 modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                             init_checkpoint)\n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        # 打印变量名\n",
    "        # logger.info(\"**** Trainable Variables ****\")\n",
    "        #\n",
    "        # # 打印加载模型的参数\n",
    "        # for var in tvars:\n",
    "        #     init_string = \"\"\n",
    "        #     if var.name in initialized_variable_names:\n",
    "        #         init_string = \", *INIT_FROM_CKPT*\"\n",
    "        #     logger.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "        #                     init_string)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            #train_op = optimizer.optimizer(total_loss, learning_rate, num_train_steps)\n",
    "            train_op = optimization.create_optimizer(\n",
    "                 total_loss, learning_rate, num_train_steps, num_warmup_steps, False)\n",
    "            hook_dict = {}\n",
    "            hook_dict['loss'] = total_loss\n",
    "            hook_dict['global_steps'] = tf.train.get_or_create_global_step()\n",
    "            logging_hook = tf.train.LoggingTensorHook(\n",
    "                hook_dict, every_n_iter=args.save_summary_steps)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op,\n",
    "                training_hooks=[logging_hook])\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            # 针对NER ,进行了修改\n",
    "            def metric_fn(label_ids, pred_ids):\n",
    "                return {\n",
    "                    \"eval_loss\": tf.metrics.mean_squared_error(labels=label_ids, predictions=pred_ids),\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, pred_ids)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metric_ops=eval_metrics\n",
    "            )\n",
    "        else:\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=pred_ids\n",
    "            )\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "# def load_data():\n",
    "#     processer = NerProcessor()\n",
    "#     processer.get_labels()\n",
    "#     example = processer.get_train_examples(FLAGS.data_dir)\n",
    "#     print()\n",
    "\n",
    "def get_last_checkpoint(model_path):\n",
    "    if not os.path.exists(os.path.join(model_path, 'checkpoint')):\n",
    "        tf.logging.info('checkpoint file not exits:'.format(os.path.join(model_path, 'checkpoint')))\n",
    "        return None\n",
    "    last = None\n",
    "    with codecs.open(os.path.join(model_path, 'checkpoint'), 'r', encoding='utf-8') as fd:\n",
    "        for line in fd:\n",
    "            line = line.strip().split(':')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            if line[0] == 'model_checkpoint_path':\n",
    "                last = line[1][2:-1]\n",
    "                break\n",
    "    return last\n",
    "\n",
    "\n",
    "def adam_filter(model_path):\n",
    "    \"\"\"\n",
    "    去掉模型中的Adam相关参数，这些参数在测试的时候是没有用的\n",
    "    :param model_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    last_name = get_last_checkpoint(model_path)\n",
    "    if last_name is None:\n",
    "        return\n",
    "    sess = tf.Session()\n",
    "    imported_meta = tf.train.import_meta_graph(os.path.join(model_path, last_name + '.meta'))\n",
    "    imported_meta.restore(sess, os.path.join(model_path, last_name))\n",
    "    need_vars = []\n",
    "    for var in tf.global_variables():\n",
    "        if 'adam_v' not in var.name and 'adam_m' not in var.name:\n",
    "            need_vars.append(var)\n",
    "    saver = tf.train.Saver(need_vars)\n",
    "    saver.save(sess, os.path.join(model_path, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 20864\n",
      "INFO:tensorflow:  Batch size = 64\n",
      "INFO:tensorflow:  Num steps = 978\n",
      "INFO:tensorflow:***** Running evaluation *****\n",
      "INFO:tensorflow:  Num examples = 2318\n",
      "INFO:tensorflow:  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "#一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "    \"ner\": NerProcessor\n",
    "}\n",
    "#载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "#检查序列的最大长度是否超出范围\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "# 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "if args.clean and args.do_train:\n",
    "    if os.path.exists(args.output_dir):\n",
    "        def del_file(path):\n",
    "            ls = os.listdir(path)\n",
    "            for i in ls:\n",
    "                c_path = os.path.join(path, i)\n",
    "                if os.path.isdir(c_path):\n",
    "                    del_file(c_path)\n",
    "                else:\n",
    "                    os.remove(c_path)\n",
    "        try:\n",
    "            del_file(args.output_dir)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pleace remove the files of output dir and data.conf')\n",
    "            exit(-1)\n",
    "\n",
    "#check output dir exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "#通过output_dir初始化数据处理类，processor\n",
    "processor = processors[args.ner](args.output_dir)\n",
    "#通过bert字典，初始化bert自带分词类\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "\n",
    "train_examples = None\n",
    "eval_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None\n",
    "\n",
    "#一般都是True\n",
    "if args.do_train and args.do_eval:\n",
    "    # 加载训练数据,train和dev，会自动拼接文件夹和train.txt\n",
    "    #返回的训练数据是一个list，每个元素是两个字符串，空格分隔字，空格分隔字标记，并写入训练examples类中\n",
    "    train_examples = processor.get_train_examples(args.data_dir)\n",
    "    #训练步数\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) *1.0 / args.batch_size * args.num_train_epochs)\n",
    "    if num_train_steps < 1:\n",
    "        raise AttributeError('training data is so small...')\n",
    "    #\n",
    "    num_warmup_steps = int(num_train_steps * args.warmup_proportion)\n",
    "\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "    tf.logging.info(\"  Batch size = %d\", args.batch_size)\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    #读取验证集\n",
    "    eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "\n",
    "    # 打印验证集数据信息\n",
    "    tf.logging.info(\"***** Running evaluation *****\")\n",
    "    tf.logging.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    tf.logging.info(\"  Batch size = %d\", args.batch_size)\n",
    "\n",
    "#获取标签集合，是一个list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-PER', 'O', 'B-LOC', 'X', 'I-LOC', 'B-PER', 'B-ORG', '[CLS]', 'I-ORG', '[SEP]'}\n"
     ]
    }
   ],
   "source": [
    "label_list = processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\project\\python_project\\bert-lstm-crf-ner\\bert\\modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\project\\python_project\\bert-lstm-crf-ner\\bert\\modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'size' to a tensor and failed. Error: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'Slice_2/packed' (op: 'Pack') with input shapes: [], [?].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'Slice_2/size' (op: 'Pack') with input shapes: [], [?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    510\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    512\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"packed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[1;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[1;32m-> 1054\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   6425\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 6426\u001b[1;33m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[0;32m   6427\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'Slice_2/size' (op: 'Pack') with input shapes: [], [?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'Slice_2/packed' (op: 'Pack') with input shapes: [], [?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    524\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[1;32m--> 525\u001b[1;33m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[0;32m    526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"packed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[1;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[1;32m-> 1054\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   6425\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 6426\u001b[1;33m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[0;32m   6427\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'Slice_2/packed' (op: 'Pack') with input shapes: [], [?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d3cd33eee369>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m total_loss, logits, trans, pred_ids, accuracy= create_model(\n\u001b[0;32m     12\u001b[0m         \u001b[0mbert_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         num_labels, False, args.dropout_rate, args.lstm_size, args.cell, args.num_layers)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#输出loss的smmary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\project\\python_project\\bert-lstm-crf-ner\\train\\models.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(bert_config, is_training, input_ids, input_mask, segment_ids, labels, num_labels, use_one_hot_embeddings, dropout_rate, lstm_size, cell, num_layers)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;31m#对原来的标签进行截断，只获取真实的id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mlabels_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mlabel_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#从坐标0,0开始切片，大小为样本大小行，lengths列\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mequal_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#把两矩阵相等的元素，转化为1，不相等为0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequal_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequal_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m   \"\"\"\n\u001b[1;32m--> 707\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   9803\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9804\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 9805\u001b[1;33m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0m\u001b[0;32m   9806\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9807\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    527\u001b[0m               raise ValueError(\n\u001b[0;32m    528\u001b[0m                   \u001b[1;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m                   (input_name, err))\n\u001b[0m\u001b[0;32m    530\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[0;32m    531\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[1;31mValueError\u001b[0m: Tried to convert 'size' to a tensor and failed. Error: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'Slice_2/packed' (op: 'Pack') with input shapes: [], [?]."
     ]
    }
   ],
   "source": [
    "with tf.name_scope('input'):\n",
    "    input_ids = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    input_mask = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    segment_ids  = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    label_ids = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "#对参数赋值，对于训练模型来说\n",
    "is_training=True\n",
    "num_labels=len(label_list) + 1\n",
    "init_checkpoint = args.init_checkpoint\n",
    "learning_rate = args.learning_rate\n",
    "total_loss, logits, trans, pred_ids, accuracy= create_model(\n",
    "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "        num_labels, False, args.dropout_rate, args.lstm_size, args.cell, args.num_layers)\n",
    "#输出loss的smmary\n",
    "tf.summary.scalar('total_loss', total_loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "#加载预训练隐变量\n",
    "tvars = tf.trainable_variables()\n",
    "# 加载BERT模型，assignmen_map，加载的预训练变量值\n",
    "if init_checkpoint:\n",
    "    (assignment_map, initialized_variable_names) = \\\n",
    "        modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                    init_checkpoint)\n",
    "    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "#优化loss\n",
    "train_op = optimization.create_optimizer(\n",
    "    total_loss, learning_rate, num_train_steps, num_warmup_steps, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(args.output_dir, \"train.tf_record\")\n",
    "#ok\n",
    "if not os.path.exists(train_file):\n",
    "    filed_based_convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer, train_file, args.output_dir)\n",
    "\n",
    "# 2.读取record 数据，组成batch，把上一部输出到文件的训练数据读取\n",
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=train_file,\n",
    "    seq_length=args.max_seq_length,\n",
    "    is_training=True,\n",
    "    drop_remainder=True,\n",
    "    batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = os.path.join(args.output_dir, \"eval.tf_record\")\n",
    "if not os.path.exists(eval_file):\n",
    "    filed_based_convert_examples_to_features(\n",
    "        eval_examples, label_list, args.max_seq_length, tokenizer, eval_file, args.output_dir)\n",
    "#构建验证集数据\n",
    "eval_input_fn = file_based_input_fn_builder(\n",
    "    input_file=eval_file,\n",
    "    seq_length=args.max_seq_length,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    batch_size=args.batch_size)\n",
    "train_input=train_input_fn.make_one_shot_iterator()\n",
    "sess = tf.InteractiveSession()\n",
    "max_step=500\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('./log', sess.graph)\n",
    "meta_train_data = train_input.get_next()\n",
    "#参数batch_size是64，train_batch_size是32，不知道train_batch_size是什么用的\n",
    "#------------------解决FailedPreconditionError:问题，初始化所有变量，不知道这样会不会影响初始化的bert预训练变量------------------\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(max_step):\n",
    "\n",
    "        #把tensor转化为numpy输入\n",
    "        train_data=sess.run([meta_train_data])[0]\n",
    "        sess.run(train_op,feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                     segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']})\n",
    "        if i%10==0:\n",
    "            train_summary, accu = sess.run([merged,accuracy], feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                     segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']})\n",
    "            train_writer.add_summary(train_summary, i)\n",
    "            print('accuracy %s at %s'%(accu,i))\n",
    "    train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
