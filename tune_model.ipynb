{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: c:\\python36\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\daizhutao\\AppData\\Roaming\\jupyter\\runtime\\kernel-78b02527-fc68-4b8d-ae10-27bc60f1c033.json\n",
      "                 ARG   VALUE\n",
      "__________________________________________________\n",
      "          batch_size = 64\n",
      "    bert_config_file = D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_config.json\n",
      "                cell = lstm\n",
      "               clean = True\n",
      "                clip = 0.5\n",
      "            data_dir = D:/project/python_project/bert-lstm-crf-ner/data_demo\n",
      "          device_map = 0\n",
      "             do_eval = True\n",
      "       do_lower_case = True\n",
      "          do_predict = True\n",
      "            do_train = True\n",
      "        dropout_rate = 0.5\n",
      "     filter_adam_var = True\n",
      "     init_checkpoint = D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\n",
      "          label_list = None\n",
      "       learning_rate = 2e-05\n",
      "           lstm_size = 128\n",
      "      max_seq_length = 128\n",
      "                 ner = ner\n",
      "          num_layers = 1\n",
      "    num_train_epochs = 3.0\n",
      "          output_dir = D:/project/python_project/bert-lstm-crf-ner/output\n",
      "save_checkpoints_steps = 500\n",
      "  save_summary_steps = 500\n",
      "           task_name = NER\n",
      "    train_batch_size = 32\n",
      "             verbose = False\n",
      "          vocab_file = D:/project/python_project/bert-lstm-crf-ner/bert\\chinese_L-12_H-768_A-12/vocab.txt\n",
      "   warmup_proportion = 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#初始化配置\n",
    "import os\n",
    "import sys\n",
    "from train.train_helper import get_args_parser\n",
    "from train.bert_lstm_ner_cg_estimator import train\n",
    "\n",
    "args = get_args_parser()\n",
    "# print(args)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "args.task_name=\"NER\"\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "args.do_predict=True\n",
    "args.data_dir=\"D:/project/python_project/bert-lstm-crf-ner/data_demo\"\n",
    "args.vocab_file=\"D:/project/python_project/bert-lstm-crf-ner/bert\\chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "args.bert_config_file=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "args.init_checkpoint=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "args.max_seq_length=128\n",
    "args.train_batch_size=32\n",
    "args.learning_rate=2e-5\n",
    "args.num_train_epochs=3.0\n",
    "args.output_dir=\"D:/project/python_project/bert-lstm-crf-ner/output\"\n",
    "#打印相关参数\n",
    "param_str = '\\n'.join(['%20s = %s' % (k, v) for k, v in sorted(vars(args).items())])\n",
    "print('usage: %s\\n%20s   %s\\n%s\\n%s\\n' % (' '.join(sys.argv), 'ARG', 'VALUE', '_' * 50, param_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "#sys.path.append('/home/idm/dzt/models/nlp_model/bert-lstm-crf-ner/')\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from train import tf_metrics\n",
    "from bert import modeling\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "# import\n",
    "\n",
    "from train.models import create_model, InputFeatures, InputExample\n",
    "\n",
    "__version__ = '0.1.0'\n",
    "\n",
    "__all__ = ['__version__', 'DataProcessor', 'NerProcessor', 'write_tokens', 'convert_single_example',\n",
    "           'filed_based_convert_examples_to_features', 'file_based_input_fn_builder',\n",
    "           'model_fn_builder', 'train']\n",
    "\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[1])\n",
    "                else:\n",
    "                    if len(contends) == 0:\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append([l, w])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    words.append('')\n",
    "                    continue\n",
    "            return lines\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        #比如set_type是train，就表示训练数据\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            #使用bert内部的tokenization包，把字符串转化成unicode\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            #模型训练的输入类，guid为唯一数据id\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines\n",
    "\n",
    "\n",
    "def write_tokens(tokens, output_dir, mode):\n",
    "    \"\"\"\n",
    "    将序列解析结果写入到文件中\n",
    "    只在mode=test的时候启用\n",
    "    :param tokens:\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if mode == \"test\":\n",
    "        path = os.path.join(output_dir, \"token_\" + mode + \".txt\")\n",
    "        wf = codecs.open(path, 'a', encoding='utf-8')\n",
    "        for token in tokens:\n",
    "            if token != \"**NULL**\":\n",
    "                wf.write(token + '\\n')\n",
    "        wf.close()\n",
    "\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode):\n",
    "    \"\"\"\n",
    "    将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
    "    :param ex_index: index\n",
    "    :param example: 一个样本\n",
    "    :param label_list: 标签列表\n",
    "    :param max_seq_length:\n",
    "    :param tokenizer:\n",
    "    :param output_dir\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label_map = {}\n",
    "    # 1表示从1开始对label进行index化\n",
    "    for (i, label) in enumerate(label_list, 1):\n",
    "        label_map[label] = i\n",
    "    # 保存label->index 的map\n",
    "    if not os.path.exists(os.path.join(output_dir, 'label2id.pkl')):\n",
    "        with codecs.open(os.path.join(output_dir, 'label2id.pkl'), 'wb') as w:\n",
    "            pickle.dump(label_map, w)\n",
    "\n",
    "    #因为从训练数据读取后，字，标签标记，都是用空格分隔的\n",
    "    textlist = example.text.split(' ')\n",
    "    labellist = example.label.split(' ')\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for i, word in enumerate(textlist):\n",
    "        # 分词，如果是中文，就是分字,但是对于一些不在BERT的vocab.txt中得字符会被进行WordPice处理（例如中文的引号），可以将所有的分字操作替换为list(input)\n",
    "        token = tokenizer.tokenize(word)\n",
    "        tokens.extend(token)\n",
    "        label_1 = labellist[i]\n",
    "        for m in range(len(token)):\n",
    "            if m == 0:\n",
    "                labels.append(label_1)\n",
    "            else:  # 一般不会出现else，因为只有一个词\n",
    "                labels.append(\"X\")\n",
    "    # tokens = tokenizer.tokenize(example.text)\n",
    "    # 序列截断\n",
    "    if len(tokens) >= max_seq_length - 1:\n",
    "        tokens = tokens[0:(max_seq_length - 2)]  # -2 的原因是因为序列需要加一个句首和句尾标志\n",
    "        labels = labels[0:(max_seq_length - 2)]\n",
    "    ntokens = []\n",
    "    segment_ids = []  #segment_ids的作用是？？？\n",
    "    label_ids = []\n",
    "    ntokens.append(\"[CLS]\")  # 句子开始设置CLS 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[CLS]\") not sure!\n",
    "    label_ids.append(label_map[\"[CLS]\"])  # O OR CLS 没有任何影响，不过我觉得O 会减少标签个数,不过句首和句尾使用不同的标志来标注，使用LCS 也没毛病\n",
    "    for i, token in enumerate(tokens):\n",
    "        ntokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(label_map[labels[i]])\n",
    "    ntokens.append(\"[SEP]\")  # 句尾添加[SEP] 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "    label_ids.append(label_map[\"[SEP]\"])\n",
    "    #调用bert内部的token2id函数，把token转化成bert使用的tokenid\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(ntokens)  # 将序列中的字(ntokens)转化为ID形式\n",
    "    input_mask = [1] * len(input_ids)   #mask是隐藏的token，用于模型训练\n",
    "    # label_mask = [1] * len(input_ids)\n",
    "    # padding, 使用\n",
    "    #小于序列长度的，进行补全操作\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        # we don't concerned about it!\n",
    "        label_ids.append(0)\n",
    "        ntokens.append(\"**NULL**\")\n",
    "        # label_mask.append(0)\n",
    "    # print(len(input_ids))\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "    # assert len(label_mask) == max_seq_length\n",
    "\n",
    "    # 打印部分样本数据信息\n",
    "    if ex_index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "            [tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "        # tf.logging.info(\"label_mask: %s\" % \" \".join([str(x) for x in label_mask]))\n",
    "\n",
    "    # 结构化为一个类，使用自定义类保存训练index化数据，\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_ids=label_ids,\n",
    "        # label_mask = label_mask\n",
    "    )\n",
    "    # mode='test'的时候才有效，把token写入文件\n",
    "    write_tokens(ntokens, output_dir, mode)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def filed_based_convert_examples_to_features(\n",
    "        examples, label_list, max_seq_length, tokenizer, output_file, output_dir, mode=None):\n",
    "    \"\"\"\n",
    "    将数据转化为TF_Record 结构，作为模型数据输入\n",
    "    :param examples:  样本\n",
    "    :param label_list:标签list\n",
    "    :param max_seq_length: 预先设定的最大序列长度\n",
    "    :param tokenizer: tokenizer 对象\n",
    "    :param output_file: tf.record 输出路径\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    # 遍历训练数据\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "        # 对于每一个训练样本,将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
    "        # 返回的是bert模型训练需要的index化token，label，mask，segment等信息\n",
    "        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()#除了记录k,v，还会记录k放入的顺序\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature(feature.label_ids)\n",
    "        # features[\"label_mask\"] = create_int_feature(feature.label_mask)\n",
    "        # tf.train.Example/Feature 是一种协议，方便序列化？？？\n",
    "        # Example中存放features特征，放入example是为了便于把特征序列化存储\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "\n",
    "def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder,batch_size):\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        # \"label_ids\":tf.VarLenFeature(tf.int64),\n",
    "        # \"label_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        #把recod变成字典example？,可能写入，读取都是按照tensorflow的某个标准\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "        d = d.repeat()\n",
    "        d = d.shuffle(buffer_size=300)\n",
    "    #通过map函数，调用_decode_record，把int64的数据转化成int32的数据，通过apply，把数据转化成batch的形式\n",
    "    d = d.apply(tf.data.experimental.map_and_batch(lambda record: _decode_record(record, name_to_features),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_parallel_calls=8,  # 并行处理数据的CPU核心数量，不要大于你机器的核心数\n",
    "                                                   drop_remainder=drop_remainder))\n",
    "    d = d.prefetch(buffer_size=4)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, args):\n",
    "    \"\"\"\n",
    "    构建模型\n",
    "    :param bert_config:\n",
    "    :param num_labels:\n",
    "    :param init_checkpoint:\n",
    "    :param learning_rate:\n",
    "    :param num_train_steps:\n",
    "    :param num_warmup_steps:\n",
    "    :param use_tpu:\n",
    "    :param use_one_hot_embeddings:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        print('shape of input_ids', input_ids.shape)\n",
    "        # label_mask = features[\"label_mask\"]\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        # 使用参数构建模型,input_idx 就是输入的样本idx表示，label_ids 就是标签的idx表示\n",
    "        #全部损失，分数，，预测类别\n",
    "        total_loss, logits, trans, pred_ids = create_model(\n",
    "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "            num_labels, False, args.dropout_rate, args.lstm_size, args.cell, args.num_layers)\n",
    "        # tf.summary.scalar('total_loss', total_loss)\n",
    "        # tf.summary.scalar('logits',logits)\n",
    "        # tf.summary.scalar('trans',trans)\n",
    "        # tf.summary.scalar('pred_ids',pred_ids)\n",
    "        #所有需要训练的变量\n",
    "        tvars = tf.trainable_variables()\n",
    "        # 加载BERT模型，assignmen_map，加载的预训练变量值\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names) = \\\n",
    "                 modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                             init_checkpoint)\n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        # 打印变量名\n",
    "        # logger.info(\"**** Trainable Variables ****\")\n",
    "        #\n",
    "        # # 打印加载模型的参数\n",
    "        # for var in tvars:\n",
    "        #     init_string = \"\"\n",
    "        #     if var.name in initialized_variable_names:\n",
    "        #         init_string = \", *INIT_FROM_CKPT*\"\n",
    "        #     logger.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "        #                     init_string)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            #train_op = optimizer.optimizer(total_loss, learning_rate, num_train_steps)\n",
    "            train_op = optimization.create_optimizer(\n",
    "                 total_loss, learning_rate, num_train_steps, num_warmup_steps, False)\n",
    "            hook_dict = {}\n",
    "            hook_dict['loss'] = total_loss\n",
    "            hook_dict['global_steps'] = tf.train.get_or_create_global_step()\n",
    "            logging_hook = tf.train.LoggingTensorHook(\n",
    "                hook_dict, every_n_iter=args.save_summary_steps)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op,\n",
    "                training_hooks=[logging_hook])\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            # 针对NER ,进行了修改\n",
    "            def metric_fn(label_ids, pred_ids):\n",
    "                return {\n",
    "                    \"eval_loss\": tf.metrics.mean_squared_error(labels=label_ids, predictions=pred_ids),\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, pred_ids)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metric_ops=eval_metrics\n",
    "            )\n",
    "        else:\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=pred_ids\n",
    "            )\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "# def load_data():\n",
    "#     processer = NerProcessor()\n",
    "#     processer.get_labels()\n",
    "#     example = processer.get_train_examples(FLAGS.data_dir)\n",
    "#     print()\n",
    "\n",
    "def get_last_checkpoint(model_path):\n",
    "    if not os.path.exists(os.path.join(model_path, 'checkpoint')):\n",
    "        tf.logging.info('checkpoint file not exits:'.format(os.path.join(model_path, 'checkpoint')))\n",
    "        return None\n",
    "    last = None\n",
    "    with codecs.open(os.path.join(model_path, 'checkpoint'), 'r', encoding='utf-8') as fd:\n",
    "        for line in fd:\n",
    "            line = line.strip().split(':')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            if line[0] == 'model_checkpoint_path':\n",
    "                last = line[1][2:-1]\n",
    "                break\n",
    "    return last\n",
    "\n",
    "\n",
    "def adam_filter(model_path):\n",
    "    \"\"\"\n",
    "    去掉模型中的Adam相关参数，这些参数在测试的时候是没有用的\n",
    "    :param model_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    last_name = get_last_checkpoint(model_path)\n",
    "    if last_name is None:\n",
    "        return\n",
    "    sess = tf.Session()\n",
    "    imported_meta = tf.train.import_meta_graph(os.path.join(model_path, last_name + '.meta'))\n",
    "    imported_meta.restore(sess, os.path.join(model_path, last_name))\n",
    "    need_vars = []\n",
    "    for var in tf.global_variables():\n",
    "        if 'adam_v' not in var.name and 'adam_m' not in var.name:\n",
    "            need_vars.append(var)\n",
    "    saver = tf.train.Saver(need_vars)\n",
    "    saver.save(sess, os.path.join(model_path, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 43\n",
      "INFO:tensorflow:  Batch size = 64\n",
      "INFO:tensorflow:  Num steps = 2\n",
      "INFO:tensorflow:***** Running evaluation *****\n",
      "INFO:tensorflow:  Num examples = 43\n",
      "INFO:tensorflow:  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "#一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "    \"ner\": NerProcessor\n",
    "}\n",
    "#载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "#检查序列的最大长度是否超出范围\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "# 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "if args.clean and args.do_train:\n",
    "    if os.path.exists(args.output_dir):\n",
    "        def del_file(path):\n",
    "            ls = os.listdir(path)\n",
    "            for i in ls:\n",
    "                c_path = os.path.join(path, i)\n",
    "                if os.path.isdir(c_path):\n",
    "                    del_file(c_path)\n",
    "                else:\n",
    "                    os.remove(c_path)\n",
    "        try:\n",
    "            del_file(args.output_dir)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pleace remove the files of output dir and data.conf')\n",
    "            exit(-1)\n",
    "\n",
    "#check output dir exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "#通过output_dir初始化数据处理类，processor\n",
    "processor = processors[args.ner](args.output_dir)\n",
    "#通过bert字典，初始化bert自带分词类\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "\n",
    "train_examples = None\n",
    "eval_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None\n",
    "\n",
    "#一般都是True\n",
    "if args.do_train and args.do_eval:\n",
    "    # 加载训练数据,train和dev，会自动拼接文件夹和train.txt\n",
    "    #返回的训练数据是一个list，每个元素是两个字符串，空格分隔字，空格分隔字标记，并写入训练examples类中\n",
    "    train_examples = processor.get_train_examples(args.data_dir)\n",
    "    #训练步数\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) *1.0 / args.batch_size * args.num_train_epochs)\n",
    "    if num_train_steps < 1:\n",
    "        raise AttributeError('training data is so small...')\n",
    "    #\n",
    "    num_warmup_steps = int(num_train_steps * args.warmup_proportion)\n",
    "\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "    tf.logging.info(\"  Batch size = %d\", args.batch_size)\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    #读取验证集\n",
    "    eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "\n",
    "    # 打印验证集数据信息\n",
    "    tf.logging.info(\"***** Running evaluation *****\")\n",
    "    tf.logging.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    tf.logging.info(\"  Batch size = %d\", args.batch_size)\n",
    "\n",
    "#获取标签集合，是一个list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[CLS]', 'B-LOC', '[SEP]', 'I-PER', 'O', 'X', 'B-PER', 'B-ORG', 'I-LOC', 'I-ORG'}\n"
     ]
    }
   ],
   "source": [
    "label_list = processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\project\\python_project\\bert-lstm-crf-ner\\bert\\modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\project\\python_project\\bert-lstm-crf-ner\\bert\\modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\training\\learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('input'):\n",
    "    input_ids = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    input_mask = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    segment_ids  = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    label_ids = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "#对参数赋值，对于训练模型来说\n",
    "is_training=True\n",
    "num_labels=len(label_list) + 1\n",
    "init_checkpoint = args.init_checkpoint\n",
    "learning_rate = args.learning_rate\n",
    "total_loss, logits, trans, pred_ids= create_model(\n",
    "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "        num_labels, False, args.dropout_rate, args.lstm_size, args.cell, args.num_layers)\n",
    "#输出loss的smmary\n",
    "tf.summary.scalar('total_loss', total_loss)\n",
    "# tf.summary.scalar('accuracy', accuracy)\n",
    "#加载预训练隐变量\n",
    "tvars = tf.trainable_variables()\n",
    "# 加载BERT模型，assignmen_map，加载的预训练变量值\n",
    "if init_checkpoint:\n",
    "    (assignment_map, initialized_variable_names) = \\\n",
    "        modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                    init_checkpoint)\n",
    "    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "#优化loss\n",
    "train_op = optimization.create_optimizer(\n",
    "    total_loss, learning_rate, num_train_steps, num_warmup_steps, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 43\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-0\n",
      "INFO:tensorflow:tokens: 海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。\n",
      "INFO:tensorflow:input_ids: 101 3862 7157 3683 6612 1765 4157 1762 1336 7305 680 7032 7305 722 7313 4638 3862 1818 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 2 9 5 2 9 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-1\n",
      "INFO:tensorflow:tokens: 这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ， 整 个 建 筑 群 精 美 而 恢 宏 。\n",
      "INFO:tensorflow:input_ids: 101 6821 2429 898 2255 988 3717 4638 1300 4289 7667 4507 1744 1079 671 3837 4638 6392 6369 2360 712 2898 6392 6369 8024 3146 702 2456 5029 5408 5125 5401 5445 2612 2131 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-2\n",
      "INFO:tensorflow:tokens: 但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 正 做 到 [UNK] 先 天 下 之 忧 而 忧 ， 后 天 下 之 乐 而 乐 [UNK] ， 淡 化 个 人 的 名 利 得 失 和 宠 辱 悲 喜 ， 把 改 革 大 业 摆 在 首 位 ， 这 样 才 能 超 越 自 我 ， 摆 脱 世 俗 ， 有 所 作 为 。\n",
      "INFO:tensorflow:input_ids: 101 852 868 711 671 702 1066 772 1054 1447 510 782 3696 1062 789 8024 2418 2496 5541 2577 2160 7333 8024 4696 3633 976 1168 100 1044 1921 678 722 2569 5445 2569 8024 1400 1921 678 722 727 5445 727 100 8024 3909 1265 702 782 4638 1399 1164 2533 1927 1469 2143 6802 2650 1599 8024 2828 3121 7484 1920 689 3030 1762 7674 855 8024 6821 3416 2798 5543 6631 6632 5632 2769 8024 3030 5564 686 921 8024 3300 2792 868 711 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-3\n",
      "INFO:tensorflow:tokens: 在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 系 的 重 要 组 成 部 分 。\n",
      "INFO:tensorflow:input_ids: 101 1762 1355 6809 1744 2157 8024 2593 3131 924 7372 1282 1146 3249 1350 8024 2347 2768 711 4852 833 924 7397 860 5143 4638 7028 6206 5299 2768 6956 1146 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-4\n",
      "INFO:tensorflow:tokens: 日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 历 史 最 佳 时 期 ， 但 其 脆 弱 性 不 言 自 明 。\n",
      "INFO:tensorflow:input_ids: 101 3189 915 697 1744 1744 1079 3124 2229 6963 1041 4007 1359 3144 8024 2226 5052 3189 915 1068 5143 4680 1184 3221 1325 1380 3297 881 3198 3309 8024 852 1071 5546 2483 2595 679 6241 5632 3209 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "train_file = os.path.join(args.output_dir, \"train.tf_record\")\n",
    "#ok\n",
    "if not os.path.exists(train_file):\n",
    "    filed_based_convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer, train_file, args.output_dir)\n",
    "\n",
    "# 2.读取record 数据，组成batch，把上一部输出到文件的训练数据读取\n",
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=train_file,\n",
    "    seq_length=args.max_seq_length,\n",
    "    is_training=True,\n",
    "    drop_remainder=True,\n",
    "    batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 43\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-0\n",
      "INFO:tensorflow:tokens: 海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。\n",
      "INFO:tensorflow:input_ids: 101 3862 7157 3683 6612 1765 4157 1762 1336 7305 680 7032 7305 722 7313 4638 3862 1818 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 2 9 5 2 9 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-1\n",
      "INFO:tensorflow:tokens: 这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ， 整 个 建 筑 群 精 美 而 恢 宏 。\n",
      "INFO:tensorflow:input_ids: 101 6821 2429 898 2255 988 3717 4638 1300 4289 7667 4507 1744 1079 671 3837 4638 6392 6369 2360 712 2898 6392 6369 8024 3146 702 2456 5029 5408 5125 5401 5445 2612 2131 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-2\n",
      "INFO:tensorflow:tokens: 但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 正 做 到 [UNK] 先 天 下 之 忧 而 忧 ， 后 天 下 之 乐 而 乐 [UNK] ， 淡 化 个 人 的 名 利 得 失 和 宠 辱 悲 喜 ， 把 改 革 大 业 摆 在 首 位 ， 这 样 才 能 超 越 自 我 ， 摆 脱 世 俗 ， 有 所 作 为 。\n",
      "INFO:tensorflow:input_ids: 101 852 868 711 671 702 1066 772 1054 1447 510 782 3696 1062 789 8024 2418 2496 5541 2577 2160 7333 8024 4696 3633 976 1168 100 1044 1921 678 722 2569 5445 2569 8024 1400 1921 678 722 727 5445 727 100 8024 3909 1265 702 782 4638 1399 1164 2533 1927 1469 2143 6802 2650 1599 8024 2828 3121 7484 1920 689 3030 1762 7674 855 8024 6821 3416 2798 5543 6631 6632 5632 2769 8024 3030 5564 686 921 8024 3300 2792 868 711 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-3\n",
      "INFO:tensorflow:tokens: 在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 系 的 重 要 组 成 部 分 。\n",
      "INFO:tensorflow:input_ids: 101 1762 1355 6809 1744 2157 8024 2593 3131 924 7372 1282 1146 3249 1350 8024 2347 2768 711 4852 833 924 7397 860 5143 4638 7028 6206 5299 2768 6956 1146 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-4\n",
      "INFO:tensorflow:tokens: 日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 历 史 最 佳 时 期 ， 但 其 脆 弱 性 不 言 自 明 。\n",
      "INFO:tensorflow:input_ids: 101 3189 915 697 1744 1744 1079 3124 2229 6963 1041 4007 1359 3144 8024 2226 5052 3189 915 1068 5143 4680 1184 3221 1325 1380 3297 881 3198 3309 8024 852 1071 5546 2483 2595 679 6241 5632 3209 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 1 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "eval_file = os.path.join(args.output_dir, \"eval.tf_record\")\n",
    "if not os.path.exists(eval_file):\n",
    "    filed_based_convert_examples_to_features(\n",
    "        eval_examples, label_list, args.max_seq_length, tokenizer, eval_file, args.output_dir)\n",
    "#构建验证集数据\n",
    "eval_input_fn = file_based_input_fn_builder(\n",
    "    input_file=eval_file,\n",
    "    seq_length=args.max_seq_length,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    batch_size=args.batch_size)\n",
    "train_input=train_input_fn.make_one_shot_iterator()\n",
    "sess = tf.InteractiveSession()\n",
    "max_step=500\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('./log', sess.graph)\n",
    "meta_train_data = train_input.get_next()\n",
    "#参数batch_size是64，train_batch_size是32，不知道train_batch_size是什么用的\n",
    "#------------------解决FailedPreconditionError:问题，初始化所有变量，不知道这样会不会影响初始化的bert预训练变量------------------\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 101 6158 6237 ...    0    0    0]\n",
      " [ 101 2769  844 ...    0    0    0]\n",
      " [ 101 5445 5307 ...    0    0    0]\n",
      " ...\n",
      " [ 101  794 1184 ... 4495  772  102]\n",
      " [ 101 3189  915 ...    0    0    0]\n",
      " [ 101 1059 1744 ... 6814 3941  102]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_step):\n",
    "\n",
    "    #把tensor转化为numpy输入\n",
    "    train_data=sess.run([meta_train_data])[0]\n",
    "    print(train_data['input_ids'])\n",
    "    sess.run(train_op,feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                 segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']})\n",
    "    if i%10==0:\n",
    "        train_summary = sess.run(merged, feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                 segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']})\n",
    "        train_writer.add_summary(train_summary, i)\n",
    "#         print('accuracy %s at %s'%(accu,i))\n",
    "        print(sess.run(tf.shape(pred_ids),feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                 segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']}))\n",
    "        print(sess.run(tf.shape(train_data['label_ids']),feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                 segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']}))\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
