{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from bert_base.train import tf_metrics\n",
    "from bert_base.bert import modeling\n",
    "from bert_base.bert import optimization\n",
    "from bert_base.bert import tokenization\n",
    "\n",
    "# import\n",
    "\n",
    "from bert_base.train.models import create_model, InputFeatures, InputExample\n",
    "\n",
    "__version__ = '0.1.0'\n",
    "\n",
    "__all__ = ['__version__', 'DataProcessor', 'NerProcessor', 'write_tokens', 'convert_single_example',\n",
    "           'filed_based_convert_examples_to_features', 'file_based_input_fn_builder',\n",
    "           'model_fn_builder', 'train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from train.train_helper import get_args_parser\n",
    "from train.bert_lstm_ner import train\n",
    "args = get_args_parser()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "args.task_name=\"NER\"\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "args.do_predict=True\n",
    "args.data_dir=\"D:/project/python_project/bert-lstm-crf-ner\\data_demo\"\n",
    "args.vocab_file=\"D:/project/python_project/bert-lstm-crf-ner/bert\\chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "args.bert_config_file=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "args.init_checkpoint=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "args.max_seq_length=128\n",
    "args.train_batch_size=32\n",
    "args.learning_rate=2e-5\n",
    "args.num_train_epochs=3.0\n",
    "args.output_dir=\"D:/project/python_project/bert-lstm-crf-ner/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[1])\n",
    "                else:\n",
    "                    if len(contends) == 0:\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append([l, w])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    words.append('')\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[1])\n",
    "                else:\n",
    "                    if len(contends) == 0:\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append([l, w])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    words.append('')\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "#一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "    \"ner\": NerProcessor\n",
    "}\n",
    "#载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "#检查序列的最大长度是否超出范围\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "# 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "if args.clean and args.do_train:\n",
    "    if os.path.exists(args.output_dir):\n",
    "        def del_file(path):\n",
    "            ls = os.listdir(path)\n",
    "            for i in ls:\n",
    "                c_path = os.path.join(path, i)\n",
    "                if os.path.isdir(c_path):\n",
    "                    del_file(c_path)\n",
    "                else:\n",
    "                    os.remove(c_path)\n",
    "\n",
    "        try:\n",
    "            del_file(args.output_dir)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pleace remove the files of output dir and data.conf')\n",
    "            exit(-1)\n",
    "\n",
    "#check output dir exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "#通过output_dir初始化数据处理类，processor\n",
    "processor = processors[args.ner](args.output_dir)\n",
    "\n",
    "#通过bert字典，初始化bert自带分词类\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#创建session的时候，对session进行配置\n",
    "session_config = tf.ConfigProto(\n",
    "    log_device_placement=False,#记录各项操作在哪台机器运行\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    allow_soft_placement=True)\n",
    "\n",
    "#estimator运行配置，包括模型保存等\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=args.output_dir,\n",
    "    save_summary_steps=500,\n",
    "    save_checkpoints_steps=500,\n",
    "    session_config=session_config\n",
    ")\n",
    "\n",
    "train_examples = None\n",
    "eval_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "    #一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "        \"ner\": NerProcessor\n",
    "    }\n",
    "    #载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "    #检查序列的最大长度是否超出范围\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "# 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "if args.clean and args.do_train:\n",
    "    if os.path.exists(args.output_dir):\n",
    "        def del_file(path):\n",
    "            ls = os.listdir(path)\n",
    "            for i in ls:\n",
    "                c_path = os.path.join(path, i)\n",
    "                if os.path.isdir(c_path):\n",
    "                    del_file(c_path)\n",
    "                else:\n",
    "                    os.remove(c_path)\n",
    "\n",
    "        try:\n",
    "            del_file(args.output_dir)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pleace remove the files of output dir and data.conf')\n",
    "            exit(-1)\n",
    "\n",
    "#check output dir exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "#通过output_dir初始化数据处理类，processor\n",
    "processor = processors[args.ner](args.output_dir)\n",
    "\n",
    "#通过bert字典，初始化bert自带分词类\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#创建session的时候，对session进行配置\n",
    "session_config = tf.ConfigProto(\n",
    "    log_device_placement=False,#记录各项操作在哪台机器运行\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    allow_soft_placement=True)\n",
    "\n",
    "#estimator运行配置，包括模型保存等\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=args.output_dir,\n",
    "    save_summary_steps=500,\n",
    "    save_checkpoints_steps=500,\n",
    "    session_config=session_config\n",
    ")\n",
    "\n",
    "train_examples = None\n",
    "eval_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/project/python_project/bert-lstm-crf-ner\\data_demo\n"
     ]
    }
   ],
   "source": [
    "print(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O O O O O O O B-LOC I-LOC O B-LOC I-LOC O O O O O O', '海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ， 整 个 建 筑 群 精 美 而 恢 宏 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 正 做 到 “ 先 天 下 之 忧 而 忧 ， 后 天 下 之 乐 而 乐 ” ， 淡 化 个 人 的 名 利 得 失 和 宠 辱 悲 喜 ， 把 改 革 大 业 摆 在 首 位 ， 这 样 才 能 超 越 自 我 ， 摆 脱 世 俗 ， 有 所 作 为 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 系 的 重 要 组 成 部 分 。'], ['B-LOC B-LOC O O O O O O O O O O O O O O B-LOC B-LOC O O O O O O O O O O O O O O O O O O O O O O', '日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 历 史 最 佳 时 期 ， 但 其 脆 弱 性 不 言 自 明 。'], ['B-PER I-PER I-PER O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O', '克 马 尔 的 女 儿 让 娜 今 年 读 五 年 级 ， 她 所 在 的 班 上 有 3 0 多 名 同 学 ， 该 班 的 “ 家 委 会 ” 由 1 0 名 家 长 组 成 。'], ['O O O O O O O O O O O O O O O O O O O O O', '参 加 步 行 的 有 男 有 女 ， 有 年 轻 人 ， 也 有 中 年 人 。'], ['B-ORG I-ORG I-ORG O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O', '沙 特 队 教 练 佩 雷 拉 ： 两 支 队 都 想 胜 ， 因 此 都 作 出 了 最 大 的 努 力 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O', '这 种 混 乱 局 面 导 致 有 些 海 域 使 用 者 的 合 法 权 益 难 以 得 到 维 护 。'], ['B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O', '鲁 宾 明 确 指 出 ， 对 政 府 的 这 种 指 控 完 全 没 有 事 实 根 据 ， 美 国 政 府 不 想 也 没 有 向 中 国 转 让 敏 感 技 术 ， 事 实 真 相 总 有 一 天 会 大 白 于 天 下 ； 众 议 院 的 这 种 做 法 令 人 “ 非 常 失 望 ” ， 将 使 美 国 的 商 业 卫 星 产 业 受 到 威 胁 ， 使 美 国 的 竞 争 力 受 到 损 害 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '体 育 场 每 天 早 6 时 至 8 时 向 群 众 免 费 开 放 ， 体 育 馆 、 游 泳 馆 等 则 增 加 综 合 服 务 ， 延 长 开 放 时 间 ， 采 取 灵 活 收 费 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '再 看 内 容 ， 图 文 并 茂 ， 简 短 的 文 字 ， 准 确 地 反 映 了 五 十 六 个 民 族 的 风 土 人 情 和 文 化 传 统 ， 把 各 民 族 的 主 要 特 点 讲 得 很 到 位 。'], ['B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '美 国 “ 哥 伦 比 亚 ” 号 航 天 飞 机 上 的 宇 航 员 今 天 一 边 进 行 实 验 ， 一 边 继 续 抢 修 出 了 故 障 的 二 氧 化 碳 清 除 装 置 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '从 前 剥 削 阶 级 在 劳 动 生 产 率 很 低 的 情 况 下 占 有 有 限 的 剩 余 产 品 ， 主 要 是 为 了 满 足 其 家 族 豪 华 生 活 的 需 要 ， 难 有 多 大 力 量 扩 大 再 生 产 ； 现 在 资 产 阶 级 在 社 会 化 大 生 产 条 件 下 榨 取 大 量 剩 余 价 值 ， 除 供 自 身 需 求 外 ， 主 要 又 转 化 为 资 本 ， 用 以 剥 削 新 的 雇 佣 劳 动 ， 从 而 使 私 人 资 本 越 来 越 扩 大 ， 生 产 资 料 也 越 来 越 集 中 。'], ['B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O B-LOC I-LOC B-LOC I-LOC I-LOC I-LOC I-LOC O O O B-ORG I-ORG I-ORG O O O O B-ORG I-ORG I-ORG O O O O B-PER I-PER I-PER O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O', '全 国 人 民 代 表 大 会 澳 门 特 别 行 政 区 筹 备 委 员 会 第 一 次 全 体 会 议 今 天 上 午 在 北 京 人 民 大 会 堂 开 幕 ， 国 务 院 副 总 理 、 筹 委 会 主 任 委 员 钱 其 琛 在 致 开 幕 词 中 指 出 ， 筹 建 澳 门 特 别 行 政 区 的 工 作 已 经 启 动 ， 筹 委 会 面 临 的 工 作 是 大 量 的 、 紧 迫 的 ， 筹 委 们 任 重 道 远 ， 希 望 大 家 齐 心 协 力 为 澳 门 的 平 稳 过 渡 、 政 权 顺 利 交 接 作 出 贡 献 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '而 经 济 社 会 的 活 动 过 程 已 经 使 整 个 经 济 学 界 深 信 ： 宏 观 经 济 的 变 化 必 须 以 微 观 经 济 为 基 础 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O', '本 病 好 发 于 面 、 颈 、 背 、 躯 干 及 外 生 殖 器 ， 可 见 于 各 种 年 龄 。'], ['B-LOC I-LOC I-LOC B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O B-LOC I-LOC I-LOC B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '北 京 市 怀 柔 县 参 试 学 生 普 遍 感 觉 第 四 节 课 饥 饿 感 消 失 了 ， 四 川 省 江 油 市 华 丰 中 学 选 用 豆 奶 和 复 合 营 养 素 后 ， 试 验 组 男 生 的 贫 血 率 下 降 1 3 个 百 分 点 ， 而 对 照 组 只 降 低 0 ． 4 4 个 百 分 点 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '党 的 宗 旨 和 当 干 部 的 目 的 决 定 了 党 员 干 部 要 乐 于 奉 献 ， 甘 愿 吃 亏 ， 吃 苦 在 前 ， 享 受 在 后 ， 把 困 难 和 危 险 留 给 自 己 ， 把 方 便 和 安 全 让 给 群 众 。'], ['O O O O O B-ORG I-ORG I-ORG O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '今 年 年 初 ， 党 中 央 、 国 务 院 根 据 国 内 外 经 济 形 势 的 变 化 ， 及 时 作 出 扩 大 内 需 、 保 持 经 济 持 续 快 速 增 长 的 重 大 决 策 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '发 现 而 定 位 ， 确 立 而 研 究 ， 一 个 文 学 流 派 就 能 提 供 许 多 富 有 意 味 的 理 论 话 题 。'], ['O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O', '党 委 书 记 粟 光 前 当 机 立 断 ： “ 我 们 都 是 他 的 家 人 ， 都 是 他 的 亲 兄 弟 ， 我 签 ！ ”'], ['O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '熟 悉 世 行 运 作 的 人 士 说 ， 贷 款 一 旦 被 推 迟 ， 将 在 几 个 月 、 甚 至 可 能 更 长 的 时 间 后 才 能 被 重 新 考 虑 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '在 我 们 的 队 伍 中 ， 确 实 有 些 只 讲 哥 们 儿 义 气 不 讲 党 的 原 则 ， 只 图 实 惠 不 讲 干 部 形 象 的 事 。'], ['O O O O O O O O O O O O O O O O O O O O O O O', '我 估 计 她 会 说 像 疤 痕 ， 像 笨 拙 女 人 手 下 的 针 线 活 儿 。'], ['B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O', '周 恩 来 总 理 说 ， 那 就 送 一 株 万 古 常 青 的 友 谊 红 杉 吧 ！'], ['O O B-LOC O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O', '在 与 巴 重 开 会 谈 问 题 上 ， 印 声 明 再 次 宣 称 ， “ 没 有 任 何 第 三 方 介 入 的 余 地 ” 。'], ['O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O', '不 久 前 ， 记 者 就 这 些 问 题 赴 江 西 省 委 老 干 部 局 进 行 了 采 访 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O', '一 是 发 动 广 大 职 工 积 极 参 与 企 业 民 主 管 理 ， 接 受 职 工 监 督 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O', '她 说 ： “ 如 今 ， 我 们 真 是 过 上 了 安 居 乐 业 的 好 日 子 。 ”'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '倘 若 对 客 观 环 境 的 变 化 反 应 迟 钝 ， 就 很 难 迅 速 地 作 出 正 确 判 断 和 决 策 ， 也 就 难 免 陷 于 被 动 甚 至 四 处 碰 壁 。'], ['O O O O O O O O O O O O O O O O O O O O O O O', '然 而 作 为 新 的 经 济 增 长 点 ， 必 将 成 为 新 的 消 费 热 点 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '被 解 雇 后 ， 失 业 者 的 首 要 任 务 是 四 处 寻 找 工 作 ， 而 联 邦 和 州 政 府 也 给 予 适 当 救 济 和 支 持 。'], ['O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O B-ORG I-ORG I-ORG I-ORG O O O', '为 达 此 目 的 ， 可 以 采 取 的 一 个 具 体 步 骤 是 吸 收 中 国 参 加 七 国 集 团 年 会 。'], ['O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '另 一 方 面 ， 龙 口 的 经 济 基 础 使 其 有 了 增 加 科 技 投 入 的 能 力 ， 只 要 真 正 重 视 人 才 、 信 息 和 创 新 能 力 的 开 发 ， 一 定 会 有 跳 跃 式 的 发 展 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '强 调 全 社 会 要 不 断 提 高 对 职 业 培 训 意 义 的 认 识 ， 将 职 业 培 训 同 教 育 和 经 济 发 展 紧 密 结 合 ， 把 中 、 小 学 九 年 基 础 教 育 变 成 继 续 升 学 或 接 受 职 业 培 训 的 准 备 阶 段 ， 让 广 大 青 少 年 懂 得 学 习 和 职 业 培 训 互 为 一 体 ， 升 学 和 职 业 培 训 没 有 贵 贱 之 分 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '请 注 意 ： 两 个 短 语 都 是 “ 动 宾 结 构 ” 而 非 “ 偏 正 结 构 ” ， 尽 孝 的 不 是 儿 女 ， 而 是 父 母 也 。'], ['O O O O O O O O O O O O O O O O O O O O', '一 些 地 方 开 始 实 行 干 部 任 用 环 保 一 票 否 决 制 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '目 前 的 技 术 条 件 和 制 作 水 平 还 缺 乏 竞 争 能 力 ， 在 国 际 电 影 制 作 业 如 摄 影 、 录 音 、 美 术 、 剪 辑 等 诸 多 方 面 的 科 技 竞 争 日 趋 激 烈 、 各 国 电 影 争 相 利 用 电 影 的 高 科 技 制 造 影 像 奇 观 以 争 夺 观 众 的 情 况 下 ， 国 产 电 影 应 该 急 起 直 追 。'], ['O O O O O O O B-PER I-PER I-PER O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O B-PER I-PER I-PER O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O', '明 代 大 医 药 学 家 李 时 珍 的 父 亲 李 言 闻 屡 试 不 第 ， 于 是 将 仕 进 的 希 望 寄 托 在 二 儿 子 李 时 珍 身 上 ， 而 李 时 珍 对 八 股 文 不 感 兴 趣 ， 对 医 药 学 特 别 酷 爱 。'], ['B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O B-PER I-PER I-PER O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-PER I-PER I-PER O B-ORG I-ORG I-ORG I-ORG I-ORG O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O O O O O O O O O O O', '全 国 人 大 常 委 会 副 委 员 长 周 光 召 、 信 息 产 业 部 部 长 吴 基 传 、 英 特 尔 公 司 董 事 长 安 德 鲁 · 葛 鲁 夫 等 应 邀 出 席 了 这 次 活 动 。'], ['O O O O O O O O O O O O O O O O O O O', '如 此 说 来 ， 这 手 推 车 也 自 然 不 是 等 闲 之 物 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O', '由 于 保 护 区 内 的 防 护 设 施 不 配 套 ， 加 上 保 护 区 本 身 管 理 上 存 在 的 问 题 ， 使 天 鹅 洲 保 护 区 难 以 起 到 应 有 的 保 护 作 用 。']]\n"
     ]
    }
   ],
   "source": [
    "lines=processor._read_data(os.path.join(args.data_dir, \"train.txt\"))\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 43\n",
      "INFO:tensorflow:  Batch size = 64\n",
      "INFO:tensorflow:  Num steps = 2\n",
      "INFO:tensorflow:***** Running evaluation *****\n",
      "INFO:tensorflow:  Num examples = 43\n",
      "INFO:tensorflow:  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "#训练步数\n",
    "num_train_steps = int(\n",
    "    len(train_examples) *1.0 / args.batch_size * args.num_train_epochs)\n",
    "if num_train_steps < 1:\n",
    "    raise AttributeError('training data is so small...')\n",
    "#\n",
    "num_warmup_steps = int(num_train_steps * args.warmup_proportion)\n",
    "\n",
    "tf.logging.info(\"***** Running training *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", args.batch_size)\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "#读取验证集\n",
    "eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "\n",
    "# 打印验证集数据信息\n",
    "tf.logging.info(\"***** Running evaluation *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(eval_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC', 'I-ORG', '[CLS]', 'B-LOC', 'O', 'B-ORG', '[SEP]', 'B-PER', 'X', 'I-PER'}\n"
     ]
    }
   ],
   "source": [
    "#获取标签集合，是一个list\n",
    "label_list = processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\bert_base\\bert\\modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\bert_base\\bert\\modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'total_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('input'):\n",
    "    input_ids = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    input_mask = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    segment_ids  = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "    label_ids = tf.placeholder(tf.int32, [None, args.max_seq_length])\n",
    "#对参数赋值，对于训练模型来说\n",
    "is_training=True\n",
    "num_labels=len(label_list) + 1\n",
    "init_checkpoint = args.init_checkpoint\n",
    "learning_rate = args.learning_rate\n",
    "total_loss, logits, trans, pred_ids = create_model(\n",
    "    bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "    num_labels, False, args.dropout_rate, args.lstm_size, args.cell, args.num_layers)\n",
    "#输出loss的smmary\n",
    "tf.summary.scalar('total_loss', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\training\\learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#加载预训练隐变量\n",
    "    tvars = tf.trainable_variables()\n",
    "    # 加载BERT模型，assignmen_map，加载的预训练变量值\n",
    "    if init_checkpoint:\n",
    "        (assignment_map, initialized_variable_names) = \\\n",
    "            modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                        init_checkpoint)\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "    #优化loss\n",
    "    train_op = optimization.create_optimizer(\n",
    "        total_loss, learning_rate, num_train_steps, num_warmup_steps, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 43\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-0\n",
      "INFO:tensorflow:tokens: 海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。\n",
      "INFO:tensorflow:input_ids: 101 3862 7157 3683 6612 1765 4157 1762 1336 7305 680 7032 7305 722 7313 4638 3862 1818 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 3 5 5 5 5 5 5 5 4 1 5 4 1 5 5 5 5 5 5 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-1\n",
      "INFO:tensorflow:tokens: 这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ， 整 个 建 筑 群 精 美 而 恢 宏 。\n",
      "INFO:tensorflow:input_ids: 101 6821 2429 898 2255 988 3717 4638 1300 4289 7667 4507 1744 1079 671 3837 4638 6392 6369 2360 712 2898 6392 6369 8024 3146 702 2456 5029 5408 5125 5401 5445 2612 2131 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-2\n",
      "INFO:tensorflow:tokens: 但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 正 做 到 [UNK] 先 天 下 之 忧 而 忧 ， 后 天 下 之 乐 而 乐 [UNK] ， 淡 化 个 人 的 名 利 得 失 和 宠 辱 悲 喜 ， 把 改 革 大 业 摆 在 首 位 ， 这 样 才 能 超 越 自 我 ， 摆 脱 世 俗 ， 有 所 作 为 。\n",
      "INFO:tensorflow:input_ids: 101 852 868 711 671 702 1066 772 1054 1447 510 782 3696 1062 789 8024 2418 2496 5541 2577 2160 7333 8024 4696 3633 976 1168 100 1044 1921 678 722 2569 5445 2569 8024 1400 1921 678 722 727 5445 727 100 8024 3909 1265 702 782 4638 1399 1164 2533 1927 1469 2143 6802 2650 1599 8024 2828 3121 7484 1920 689 3030 1762 7674 855 8024 6821 3416 2798 5543 6631 6632 5632 2769 8024 3030 5564 686 921 8024 3300 2792 868 711 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-3\n",
      "INFO:tensorflow:tokens: 在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 系 的 重 要 组 成 部 分 。\n",
      "INFO:tensorflow:input_ids: 101 1762 1355 6809 1744 2157 8024 2593 3131 924 7372 1282 1146 3249 1350 8024 2347 2768 711 4852 833 924 7397 860 5143 4638 7028 6206 5299 2768 6956 1146 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-4\n",
      "INFO:tensorflow:tokens: 日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 历 史 最 佳 时 期 ， 但 其 脆 弱 性 不 言 自 明 。\n",
      "INFO:tensorflow:input_ids: 101 3189 915 697 1744 1744 1079 3124 2229 6963 1041 4007 1359 3144 8024 2226 5052 3189 915 1068 5143 4680 1184 3221 1325 1380 3297 881 3198 3309 8024 852 1071 5546 2483 2595 679 6241 5632 3209 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 3 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# 1. 将数据转化为tf_record 数据,并把训练数据序列化，并写出到文件\n",
    "train_file = os.path.join(args.output_dir, \"train.tf_record\")\n",
    "#ok\n",
    "if not os.path.exists(train_file):\n",
    "    filed_based_convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer, train_file, args.output_dir)\n",
    "\n",
    "# 2.读取record 数据，组成batch，把上一部输出到文件的训练数据读取\n",
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=train_file,\n",
    "    seq_length=args.max_seq_length,\n",
    "    is_training=True,\n",
    "    drop_remainder=True,\n",
    "    batch_size=args.batch_size)\n",
    "# estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "eval_file = os.path.join(args.output_dir, \"eval.tf_record\")\n",
    "if not os.path.exists(eval_file):\n",
    "    filed_based_convert_examples_to_features(\n",
    "        eval_examples, label_list, args.max_seq_length, tokenizer, eval_file, args.output_dir)\n",
    "#构建验证集数据\n",
    "eval_input_fn = file_based_input_fn_builder(\n",
    "    input_file=eval_file,\n",
    "    seq_length=args.max_seq_length,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "train_input=train_input_fn.make_one_shot_iterator()\n",
    "sess = tf.InteractiveSession()\n",
    "max_step=100\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('./log', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "meta_train_data = train_input.get_next()\n",
    "train_data=sess.run([meta_train_data])\n",
    "print(len(train_data))\n",
    "# for t in train_data:\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Saving summary loss at 0\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "meta_train_data = train_input.get_next()\n",
    "#一个batch的数据大小是64，repeat后，batch=32*2\n",
    "for i in range(max_step):\n",
    "\n",
    "    #把tensor转化为numpy输入\n",
    "    train_data=sess.run([meta_train_data])[0]\n",
    "    sess.run(train_op,feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                 segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']})\n",
    "    if i%10==0:\n",
    "        train_summary = sess.run(merged, feed_dict={input_ids:train_data['input_ids'],input_mask:train_data['input_mask'],\n",
    "                                 segment_ids:train_data['segment_ids'],label_ids:train_data['label_ids']})\n",
    "        train_writer.add_summary(train_summary, i)\n",
    "        print('Saving summary loss at %s'%(i))\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
