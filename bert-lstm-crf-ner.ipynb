{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from bert_base.train import tf_metrics\n",
    "from bert_base.bert import modeling\n",
    "from bert_base.bert import optimization\n",
    "from bert_base.bert import tokenization\n",
    "\n",
    "# import\n",
    "\n",
    "from bert_base.train.models import create_model, InputFeatures, InputExample\n",
    "\n",
    "__version__ = '0.1.0'\n",
    "\n",
    "__all__ = ['__version__', 'DataProcessor', 'NerProcessor', 'write_tokens', 'convert_single_example',\n",
    "           'filed_based_convert_examples_to_features', 'file_based_input_fn_builder',\n",
    "           'model_fn_builder', 'train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from train.train_helper import get_args_parser\n",
    "from train.bert_lstm_ner import train\n",
    "args = get_args_parser()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "args.task_name=\"NER\"\n",
    "args.do_train=True\n",
    "args.do_eval=True\n",
    "args.do_predict=True\n",
    "args.data_dir=\"D:/project/python_project/bert-lstm-crf-ner\\data_demo\"\n",
    "args.vocab_file=\"D:/project/python_project/bert-lstm-crf-ner/bert\\chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "args.bert_config_file=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "args.init_checkpoint=\"D:/project/python_project/bert-lstm-crf-ner/bert/chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "args.max_seq_length=128\n",
    "args.train_batch_size=32\n",
    "args.learning_rate=2e-5\n",
    "args.num_train_epochs=3.0\n",
    "args.output_dir=\"D:/project/python_project/bert-lstm-crf-ner/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[1])\n",
    "                else:\n",
    "                    if len(contends) == 0:\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append([l, w])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    words.append('')\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[1])\n",
    "                else:\n",
    "                    if len(contends) == 0:\n",
    "                        l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                        w = ' '.join([word for word in words if len(word) > 0])\n",
    "                        lines.append([l, w])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    words.append('')\n",
    "                    continue\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, output_dir):\n",
    "        self.labels = set()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")), \"train\"\n",
    "        )\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.txt\")), \"dev\"\n",
    "        )\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self, labels=None):\n",
    "        #传入参数可能是labels文件路径，也可能是逗号分隔的labels文本\n",
    "        if labels is not None:\n",
    "            try:\n",
    "                # 支持从文件中读取标签类型\n",
    "                if os.path.exists(labels) and os.path.isfile(labels):\n",
    "                    with codecs.open(labels, 'r', encoding='utf-8') as fd:\n",
    "                        for line in fd:\n",
    "                            self.labels.append(line.strip())\n",
    "                else:\n",
    "                    # 否则通过传入的参数，按照逗号分割\n",
    "                    self.labels = labels.split(',')\n",
    "                self.labels = set(self.labels) # to set\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # 通过读取train文件获取标签的方法会出现一定的风险。\n",
    "        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):\n",
    "            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:\n",
    "                self.labels = pickle.load(rf)\n",
    "        else:\n",
    "            if len(self.labels) > 0:\n",
    "                #pkl文件不存在，就按照读取的标签集合加上一些其他标签，写入pkl\n",
    "                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n",
    "                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:\n",
    "                    pickle.dump(self.labels, rf)\n",
    "            else:\n",
    "                #如果什么都没有，都按照代码写好的标签集合\n",
    "                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n",
    "        return self.labels\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text = tokenization.convert_to_unicode(line[1])\n",
    "            label = tokenization.convert_to_unicode(line[0])\n",
    "            # if i == 0:\n",
    "            #     print('label: ', label)\n",
    "            examples.append(InputExample(guid=guid, text=text, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _read_data(self, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                contends = line.strip()\n",
    "                tokens = contends.split(' ')\n",
    "                if len(tokens) == 2:\n",
    "                    words.append(tokens[0])\n",
    "                    labels.append(tokens[-1])\n",
    "                else:\n",
    "                    if len(contends) == 0 and len(words) > 0:\n",
    "                        label = []\n",
    "                        word = []\n",
    "                        for l, w in zip(labels, words):\n",
    "                            if len(l) > 0 and len(w) > 0:\n",
    "                                label.append(l)\n",
    "                                self.labels.add(l)\n",
    "                                word.append(w)\n",
    "                        lines.append([' '.join(label), ' '.join(word)])\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        continue\n",
    "                if contends.startswith(\"-DOCSTART-\"):\n",
    "                    continue\n",
    "            return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "#一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "    \"ner\": NerProcessor\n",
    "}\n",
    "#载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "#检查序列的最大长度是否超出范围\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "# 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "if args.clean and args.do_train:\n",
    "    if os.path.exists(args.output_dir):\n",
    "        def del_file(path):\n",
    "            ls = os.listdir(path)\n",
    "            for i in ls:\n",
    "                c_path = os.path.join(path, i)\n",
    "                if os.path.isdir(c_path):\n",
    "                    del_file(c_path)\n",
    "                else:\n",
    "                    os.remove(c_path)\n",
    "\n",
    "        try:\n",
    "            del_file(args.output_dir)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pleace remove the files of output dir and data.conf')\n",
    "            exit(-1)\n",
    "\n",
    "#check output dir exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "#通过output_dir初始化数据处理类，processor\n",
    "processor = processors[args.ner](args.output_dir)\n",
    "\n",
    "#通过bert字典，初始化bert自带分词类\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#创建session的时候，对session进行配置\n",
    "session_config = tf.ConfigProto(\n",
    "    log_device_placement=False,#记录各项操作在哪台机器运行\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    allow_soft_placement=True)\n",
    "\n",
    "#estimator运行配置，包括模型保存等\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=args.output_dir,\n",
    "    save_summary_steps=500,\n",
    "    save_checkpoints_steps=500,\n",
    "    session_config=session_config\n",
    ")\n",
    "\n",
    "train_examples = None\n",
    "eval_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device_map\n",
    "\n",
    "    #一个处理的类，包括训练数据的输入等\n",
    "processors = {\n",
    "        \"ner\": NerProcessor\n",
    "    }\n",
    "    #载入bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "    #检查序列的最大长度是否超出范围\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "# 在re train 的时候，才删除上一轮产出的文件，在predicted 的时候不做clean\n",
    "if args.clean and args.do_train:\n",
    "    if os.path.exists(args.output_dir):\n",
    "        def del_file(path):\n",
    "            ls = os.listdir(path)\n",
    "            for i in ls:\n",
    "                c_path = os.path.join(path, i)\n",
    "                if os.path.isdir(c_path):\n",
    "                    del_file(c_path)\n",
    "                else:\n",
    "                    os.remove(c_path)\n",
    "\n",
    "        try:\n",
    "            del_file(args.output_dir)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pleace remove the files of output dir and data.conf')\n",
    "            exit(-1)\n",
    "\n",
    "#check output dir exists\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "#通过output_dir初始化数据处理类，processor\n",
    "processor = processors[args.ner](args.output_dir)\n",
    "\n",
    "#通过bert字典，初始化bert自带分词类\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#创建session的时候，对session进行配置\n",
    "session_config = tf.ConfigProto(\n",
    "    log_device_placement=False,#记录各项操作在哪台机器运行\n",
    "    inter_op_parallelism_threads=0,\n",
    "    intra_op_parallelism_threads=0,\n",
    "    allow_soft_placement=True)\n",
    "\n",
    "#estimator运行配置，包括模型保存等\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=args.output_dir,\n",
    "    save_summary_steps=500,\n",
    "    save_checkpoints_steps=500,\n",
    "    session_config=session_config\n",
    ")\n",
    "\n",
    "train_examples = None\n",
    "eval_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/project/python_project/bert-lstm-crf-ner\\data_demo\n"
     ]
    }
   ],
   "source": [
    "print(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O O O O O O O B-LOC I-LOC O B-LOC I-LOC O O O O O O', '海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ， 整 个 建 筑 群 精 美 而 恢 宏 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 正 做 到 “ 先 天 下 之 忧 而 忧 ， 后 天 下 之 乐 而 乐 ” ， 淡 化 个 人 的 名 利 得 失 和 宠 辱 悲 喜 ， 把 改 革 大 业 摆 在 首 位 ， 这 样 才 能 超 越 自 我 ， 摆 脱 世 俗 ， 有 所 作 为 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 系 的 重 要 组 成 部 分 。'], ['B-LOC B-LOC O O O O O O O O O O O O O O B-LOC B-LOC O O O O O O O O O O O O O O O O O O O O O O', '日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 历 史 最 佳 时 期 ， 但 其 脆 弱 性 不 言 自 明 。'], ['B-PER I-PER I-PER O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O', '克 马 尔 的 女 儿 让 娜 今 年 读 五 年 级 ， 她 所 在 的 班 上 有 3 0 多 名 同 学 ， 该 班 的 “ 家 委 会 ” 由 1 0 名 家 长 组 成 。'], ['O O O O O O O O O O O O O O O O O O O O O', '参 加 步 行 的 有 男 有 女 ， 有 年 轻 人 ， 也 有 中 年 人 。'], ['B-ORG I-ORG I-ORG O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O', '沙 特 队 教 练 佩 雷 拉 ： 两 支 队 都 想 胜 ， 因 此 都 作 出 了 最 大 的 努 力 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O', '这 种 混 乱 局 面 导 致 有 些 海 域 使 用 者 的 合 法 权 益 难 以 得 到 维 护 。'], ['B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O', '鲁 宾 明 确 指 出 ， 对 政 府 的 这 种 指 控 完 全 没 有 事 实 根 据 ， 美 国 政 府 不 想 也 没 有 向 中 国 转 让 敏 感 技 术 ， 事 实 真 相 总 有 一 天 会 大 白 于 天 下 ； 众 议 院 的 这 种 做 法 令 人 “ 非 常 失 望 ” ， 将 使 美 国 的 商 业 卫 星 产 业 受 到 威 胁 ， 使 美 国 的 竞 争 力 受 到 损 害 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '体 育 场 每 天 早 6 时 至 8 时 向 群 众 免 费 开 放 ， 体 育 馆 、 游 泳 馆 等 则 增 加 综 合 服 务 ， 延 长 开 放 时 间 ， 采 取 灵 活 收 费 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '再 看 内 容 ， 图 文 并 茂 ， 简 短 的 文 字 ， 准 确 地 反 映 了 五 十 六 个 民 族 的 风 土 人 情 和 文 化 传 统 ， 把 各 民 族 的 主 要 特 点 讲 得 很 到 位 。'], ['B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '美 国 “ 哥 伦 比 亚 ” 号 航 天 飞 机 上 的 宇 航 员 今 天 一 边 进 行 实 验 ， 一 边 继 续 抢 修 出 了 故 障 的 二 氧 化 碳 清 除 装 置 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '从 前 剥 削 阶 级 在 劳 动 生 产 率 很 低 的 情 况 下 占 有 有 限 的 剩 余 产 品 ， 主 要 是 为 了 满 足 其 家 族 豪 华 生 活 的 需 要 ， 难 有 多 大 力 量 扩 大 再 生 产 ； 现 在 资 产 阶 级 在 社 会 化 大 生 产 条 件 下 榨 取 大 量 剩 余 价 值 ， 除 供 自 身 需 求 外 ， 主 要 又 转 化 为 资 本 ， 用 以 剥 削 新 的 雇 佣 劳 动 ， 从 而 使 私 人 资 本 越 来 越 扩 大 ， 生 产 资 料 也 越 来 越 集 中 。'], ['B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O B-LOC I-LOC B-LOC I-LOC I-LOC I-LOC I-LOC O O O B-ORG I-ORG I-ORG O O O O B-ORG I-ORG I-ORG O O O O B-PER I-PER I-PER O O O O O O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O', '全 国 人 民 代 表 大 会 澳 门 特 别 行 政 区 筹 备 委 员 会 第 一 次 全 体 会 议 今 天 上 午 在 北 京 人 民 大 会 堂 开 幕 ， 国 务 院 副 总 理 、 筹 委 会 主 任 委 员 钱 其 琛 在 致 开 幕 词 中 指 出 ， 筹 建 澳 门 特 别 行 政 区 的 工 作 已 经 启 动 ， 筹 委 会 面 临 的 工 作 是 大 量 的 、 紧 迫 的 ， 筹 委 们 任 重 道 远 ， 希 望 大 家 齐 心 协 力 为 澳 门 的 平 稳 过 渡 、 政 权 顺 利 交 接 作 出 贡 献 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '而 经 济 社 会 的 活 动 过 程 已 经 使 整 个 经 济 学 界 深 信 ： 宏 观 经 济 的 变 化 必 须 以 微 观 经 济 为 基 础 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O', '本 病 好 发 于 面 、 颈 、 背 、 躯 干 及 外 生 殖 器 ， 可 见 于 各 种 年 龄 。'], ['B-LOC I-LOC I-LOC B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O B-LOC I-LOC I-LOC B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '北 京 市 怀 柔 县 参 试 学 生 普 遍 感 觉 第 四 节 课 饥 饿 感 消 失 了 ， 四 川 省 江 油 市 华 丰 中 学 选 用 豆 奶 和 复 合 营 养 素 后 ， 试 验 组 男 生 的 贫 血 率 下 降 1 3 个 百 分 点 ， 而 对 照 组 只 降 低 0 ． 4 4 个 百 分 点 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '党 的 宗 旨 和 当 干 部 的 目 的 决 定 了 党 员 干 部 要 乐 于 奉 献 ， 甘 愿 吃 亏 ， 吃 苦 在 前 ， 享 受 在 后 ， 把 困 难 和 危 险 留 给 自 己 ， 把 方 便 和 安 全 让 给 群 众 。'], ['O O O O O B-ORG I-ORG I-ORG O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '今 年 年 初 ， 党 中 央 、 国 务 院 根 据 国 内 外 经 济 形 势 的 变 化 ， 及 时 作 出 扩 大 内 需 、 保 持 经 济 持 续 快 速 增 长 的 重 大 决 策 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '发 现 而 定 位 ， 确 立 而 研 究 ， 一 个 文 学 流 派 就 能 提 供 许 多 富 有 意 味 的 理 论 话 题 。'], ['O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O', '党 委 书 记 粟 光 前 当 机 立 断 ： “ 我 们 都 是 他 的 家 人 ， 都 是 他 的 亲 兄 弟 ， 我 签 ！ ”'], ['O O B-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '熟 悉 世 行 运 作 的 人 士 说 ， 贷 款 一 旦 被 推 迟 ， 将 在 几 个 月 、 甚 至 可 能 更 长 的 时 间 后 才 能 被 重 新 考 虑 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '在 我 们 的 队 伍 中 ， 确 实 有 些 只 讲 哥 们 儿 义 气 不 讲 党 的 原 则 ， 只 图 实 惠 不 讲 干 部 形 象 的 事 。'], ['O O O O O O O O O O O O O O O O O O O O O O O', '我 估 计 她 会 说 像 疤 痕 ， 像 笨 拙 女 人 手 下 的 针 线 活 儿 。'], ['B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O', '周 恩 来 总 理 说 ， 那 就 送 一 株 万 古 常 青 的 友 谊 红 杉 吧 ！'], ['O O B-LOC O O O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O', '在 与 巴 重 开 会 谈 问 题 上 ， 印 声 明 再 次 宣 称 ， “ 没 有 任 何 第 三 方 介 入 的 余 地 ” 。'], ['O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O', '不 久 前 ， 记 者 就 这 些 问 题 赴 江 西 省 委 老 干 部 局 进 行 了 采 访 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O', '一 是 发 动 广 大 职 工 积 极 参 与 企 业 民 主 管 理 ， 接 受 职 工 监 督 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O', '她 说 ： “ 如 今 ， 我 们 真 是 过 上 了 安 居 乐 业 的 好 日 子 。 ”'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '倘 若 对 客 观 环 境 的 变 化 反 应 迟 钝 ， 就 很 难 迅 速 地 作 出 正 确 判 断 和 决 策 ， 也 就 难 免 陷 于 被 动 甚 至 四 处 碰 壁 。'], ['O O O O O O O O O O O O O O O O O O O O O O O', '然 而 作 为 新 的 经 济 增 长 点 ， 必 将 成 为 新 的 消 费 热 点 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '被 解 雇 后 ， 失 业 者 的 首 要 任 务 是 四 处 寻 找 工 作 ， 而 联 邦 和 州 政 府 也 给 予 适 当 救 济 和 支 持 。'], ['O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O B-ORG I-ORG I-ORG I-ORG O O O', '为 达 此 目 的 ， 可 以 采 取 的 一 个 具 体 步 骤 是 吸 收 中 国 参 加 七 国 集 团 年 会 。'], ['O O O O O B-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '另 一 方 面 ， 龙 口 的 经 济 基 础 使 其 有 了 增 加 科 技 投 入 的 能 力 ， 只 要 真 正 重 视 人 才 、 信 息 和 创 新 能 力 的 开 发 ， 一 定 会 有 跳 跃 式 的 发 展 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '强 调 全 社 会 要 不 断 提 高 对 职 业 培 训 意 义 的 认 识 ， 将 职 业 培 训 同 教 育 和 经 济 发 展 紧 密 结 合 ， 把 中 、 小 学 九 年 基 础 教 育 变 成 继 续 升 学 或 接 受 职 业 培 训 的 准 备 阶 段 ， 让 广 大 青 少 年 懂 得 学 习 和 职 业 培 训 互 为 一 体 ， 升 学 和 职 业 培 训 没 有 贵 贱 之 分 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '请 注 意 ： 两 个 短 语 都 是 “ 动 宾 结 构 ” 而 非 “ 偏 正 结 构 ” ， 尽 孝 的 不 是 儿 女 ， 而 是 父 母 也 。'], ['O O O O O O O O O O O O O O O O O O O O', '一 些 地 方 开 始 实 行 干 部 任 用 环 保 一 票 否 决 制 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', '目 前 的 技 术 条 件 和 制 作 水 平 还 缺 乏 竞 争 能 力 ， 在 国 际 电 影 制 作 业 如 摄 影 、 录 音 、 美 术 、 剪 辑 等 诸 多 方 面 的 科 技 竞 争 日 趋 激 烈 、 各 国 电 影 争 相 利 用 电 影 的 高 科 技 制 造 影 像 奇 观 以 争 夺 观 众 的 情 况 下 ， 国 产 电 影 应 该 急 起 直 追 。'], ['O O O O O O O B-PER I-PER I-PER O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O B-PER I-PER I-PER O O O O B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O', '明 代 大 医 药 学 家 李 时 珍 的 父 亲 李 言 闻 屡 试 不 第 ， 于 是 将 仕 进 的 希 望 寄 托 在 二 儿 子 李 时 珍 身 上 ， 而 李 时 珍 对 八 股 文 不 感 兴 趣 ， 对 医 药 学 特 别 酷 爱 。'], ['B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O O B-PER I-PER I-PER O B-ORG I-ORG I-ORG I-ORG I-ORG O O B-PER I-PER I-PER O B-ORG I-ORG I-ORG I-ORG I-ORG O O O B-PER I-PER I-PER I-PER I-PER I-PER I-PER O O O O O O O O O O O', '全 国 人 大 常 委 会 副 委 员 长 周 光 召 、 信 息 产 业 部 部 长 吴 基 传 、 英 特 尔 公 司 董 事 长 安 德 鲁 · 葛 鲁 夫 等 应 邀 出 席 了 这 次 活 动 。'], ['O O O O O O O O O O O O O O O O O O O', '如 此 说 来 ， 这 手 推 车 也 自 然 不 是 等 闲 之 物 。'], ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O', '由 于 保 护 区 内 的 防 护 设 施 不 配 套 ， 加 上 保 护 区 本 身 管 理 上 存 在 的 问 题 ， 使 天 鹅 洲 保 护 区 难 以 起 到 应 有 的 保 护 作 用 。']]\n"
     ]
    }
   ],
   "source": [
    "lines=processor._read_data(os.path.join(args.data_dir, \"train.txt\"))\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 43\n",
      "INFO:tensorflow:  Batch size = 64\n",
      "INFO:tensorflow:  Num steps = 2\n",
      "INFO:tensorflow:***** Running evaluation *****\n",
      "INFO:tensorflow:  Num examples = 43\n",
      "INFO:tensorflow:  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "#训练步数\n",
    "num_train_steps = int(\n",
    "    len(train_examples) *1.0 / args.batch_size * args.num_train_epochs)\n",
    "if num_train_steps < 1:\n",
    "    raise AttributeError('training data is so small...')\n",
    "#\n",
    "num_warmup_steps = int(num_train_steps * args.warmup_proportion)\n",
    "\n",
    "tf.logging.info(\"***** Running training *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", args.batch_size)\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "#读取验证集\n",
    "eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "\n",
    "# 打印验证集数据信息\n",
    "tf.logging.info(\"***** Running evaluation *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(eval_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC', 'X', '[SEP]', 'B-LOC', '[CLS]', 'O', 'I-PER', 'B-ORG', 'I-ORG', 'B-PER'}\n"
     ]
    }
   ],
   "source": [
    "#获取标签集合，是一个list\n",
    "label_list = processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, args):\n",
    "    \"\"\"\n",
    "    构建模型\n",
    "    :param bert_config:\n",
    "    :param num_labels:\n",
    "    :param init_checkpoint:\n",
    "    :param learning_rate:\n",
    "    :param num_train_steps:\n",
    "    :param num_warmup_steps:\n",
    "    :param use_tpu:\n",
    "    :param use_one_hot_embeddings:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        print('shape of input_ids', input_ids.shape)\n",
    "        # label_mask = features[\"label_mask\"]\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        # 使用参数构建模型,input_idx 就是输入的样本idx表示，label_ids 就是标签的idx表示\n",
    "        #全部损失，分数，，预测类别\n",
    "        total_loss, logits, trans, pred_ids = create_model(\n",
    "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "            num_labels, False, args.dropout_rate, args.lstm_size, args.cell, args.num_layers)\n",
    "        tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "        #所有需要训练的变量\n",
    "        tvars = tf.trainable_variables()\n",
    "        # 加载BERT模型，assignmen_map，加载的预训练变量值\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names) = \\\n",
    "                 modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                             init_checkpoint)\n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        # 打印变量名\n",
    "        # logger.info(\"**** Trainable Variables ****\")\n",
    "        #\n",
    "        # # 打印加载模型的参数\n",
    "        # for var in tvars:\n",
    "        #     init_string = \"\"\n",
    "        #     if var.name in initialized_variable_names:\n",
    "        #         init_string = \", *INIT_FROM_CKPT*\"\n",
    "        #     logger.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "        #                     init_string)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            #train_op = optimizer.optimizer(total_loss, learning_rate, num_train_steps)\n",
    "            train_op = optimization.create_optimizer(\n",
    "                 total_loss, learning_rate, num_train_steps, num_warmup_steps, False)\n",
    "            hook_dict = {}\n",
    "            hook_dict['loss'] = total_loss\n",
    "            hook_dict['global_steps'] = tf.train.get_or_create_global_step()\n",
    "            logging_hook = tf.train.LoggingTensorHook(\n",
    "                hook_dict, every_n_iter=args.save_summary_steps)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op,\n",
    "                training_hooks=[logging_hook])\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            # 针对NER ,进行了修改\n",
    "            def metric_fn(label_ids, pred_ids):\n",
    "                return {\n",
    "                    \"eval_loss\": tf.metrics.mean_squared_error(labels=label_ids, predictions=pred_ids),\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, pred_ids)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metric_ops=eval_metrics\n",
    "            )\n",
    "        else:\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=pred_ids\n",
    "            )\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'D:/project/python_project/bert-lstm-crf-ner/output', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000181F8D9B390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    " label_list = processor.get_labels()\n",
    "# 返回的model_dn 是一个函数，其定义了模型，训练，评测方法，并且使用钩子参数，加载了BERT模型的参数进行了自己模型的参数初始化过程\n",
    "# tf 新的架构方法，通过定义model_fn 函数，定义模型，然后通过EstimatorAPI进行模型的其他工作，Es就可以控制模型的训练，预测，评估工作等。\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    num_labels=len(label_list) + 1,\n",
    "    init_checkpoint=args.init_checkpoint,\n",
    "    learning_rate=args.learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    args=args)\n",
    "\n",
    "params = {\n",
    "    'batch_size': args.batch_size\n",
    "}\n",
    "#把自定义模型函数，参数，配置，都放入Estimator中\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn,\n",
    "    params=params,#主要定义了batch_size的大小\n",
    "    config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(args.output_dir, \"train.tf_record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode):\n",
    "    \"\"\"\n",
    "    将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
    "    :param ex_index: index\n",
    "    :param example: 一个样本\n",
    "    :param label_list: 标签列表\n",
    "    :param max_seq_length:\n",
    "    :param tokenizer:\n",
    "    :param output_dir\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label_map = {}\n",
    "    # 1表示从1开始对label进行index化\n",
    "    for (i, label) in enumerate(label_list, 1):\n",
    "        label_map[label] = i\n",
    "    # 保存label->index 的map\n",
    "    if not os.path.exists(os.path.join(output_dir, 'label2id.pkl')):\n",
    "        with codecs.open(os.path.join(output_dir, 'label2id.pkl'), 'wb') as w:\n",
    "            pickle.dump(label_map, w)\n",
    "\n",
    "    #因为从训练数据读取后，字，标签标记，都是用空格分隔的\n",
    "    textlist = example.text.split(' ')\n",
    "    labellist = example.label.split(' ')\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for i, word in enumerate(textlist):\n",
    "        # 分词，如果是中文，就是分字,但是对于一些不在BERT的vocab.txt中得字符会被进行WordPice处理（例如中文的引号），可以将所有的分字操作替换为list(input)\n",
    "        token = tokenizer.tokenize(word)\n",
    "        tokens.extend(token)\n",
    "        label_1 = labellist[i]\n",
    "        for m in range(len(token)):\n",
    "            if m == 0:\n",
    "                labels.append(label_1)\n",
    "            else:  # 一般不会出现else，因为只有一个词\n",
    "                labels.append(\"X\")\n",
    "    # tokens = tokenizer.tokenize(example.text)\n",
    "    # 序列截断\n",
    "    if len(tokens) >= max_seq_length - 1:\n",
    "        tokens = tokens[0:(max_seq_length - 2)]  # -2 的原因是因为序列需要加一个句首和句尾标志\n",
    "        labels = labels[0:(max_seq_length - 2)]\n",
    "    ntokens = []\n",
    "    segment_ids = []\n",
    "    label_ids = []\n",
    "    ntokens.append(\"[CLS]\")  # 句子开始设置CLS 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[CLS]\") not sure!\n",
    "    label_ids.append(label_map[\"[CLS]\"])  # O OR CLS 没有任何影响，不过我觉得O 会减少标签个数,不过句首和句尾使用不同的标志来标注，使用LCS 也没毛病\n",
    "    for i, token in enumerate(tokens):\n",
    "        ntokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(label_map[labels[i]])\n",
    "    ntokens.append(\"[SEP]\")  # 句尾添加[SEP] 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "    label_ids.append(label_map[\"[SEP]\"])\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(ntokens)  # 将序列中的字(ntokens)转化为ID形式\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    # label_mask = [1] * len(input_ids)\n",
    "    # padding, 使用\n",
    "    #小于序列长度的，进行补全操作\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        # we don't concerned about it!\n",
    "        label_ids.append(0)\n",
    "        ntokens.append(\"**NULL**\")\n",
    "        # label_mask.append(0)\n",
    "    # print(len(input_ids))\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "    # assert len(label_mask) == max_seq_length\n",
    "\n",
    "    # 打印部分样本数据信息\n",
    "    if ex_index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "            [tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "        # tf.logging.info(\"label_mask: %s\" % \" \".join([str(x) for x in label_mask]))\n",
    "\n",
    "    # 结构化为一个类\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_ids=label_ids,\n",
    "        # label_mask = label_mask\n",
    "    )\n",
    "    # mode='test'的时候才有效\n",
    "    write_tokens(ntokens, output_dir, mode)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode):\n",
    "    \"\"\"\n",
    "    将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
    "    :param ex_index: index\n",
    "    :param example: 一个样本\n",
    "    :param label_list: 标签列表\n",
    "    :param max_seq_length:\n",
    "    :param tokenizer:\n",
    "    :param output_dir\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label_map = {}\n",
    "    # 1表示从1开始对label进行index化\n",
    "    for (i, label) in enumerate(label_list, 1):\n",
    "        label_map[label] = i\n",
    "    # 保存label->index 的map\n",
    "    if not os.path.exists(os.path.join(output_dir, 'label2id.pkl')):\n",
    "        with codecs.open(os.path.join(output_dir, 'label2id.pkl'), 'wb') as w:\n",
    "            pickle.dump(label_map, w)\n",
    "\n",
    "    #因为从训练数据读取后，字，标签标记，都是用空格分隔的\n",
    "    textlist = example.text.split(' ')\n",
    "    labellist = example.label.split(' ')\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for i, word in enumerate(textlist):\n",
    "        # 分词，如果是中文，就是分字,但是对于一些不在BERT的vocab.txt中得字符会被进行WordPice处理（例如中文的引号），可以将所有的分字操作替换为list(input)\n",
    "        token = tokenizer.tokenize(word)\n",
    "        tokens.extend(token)\n",
    "        label_1 = labellist[i]\n",
    "        for m in range(len(token)):\n",
    "            if m == 0:\n",
    "                labels.append(label_1)\n",
    "            else:  # 一般不会出现else，因为只有一个词\n",
    "                labels.append(\"X\")\n",
    "    # tokens = tokenizer.tokenize(example.text)\n",
    "    # 序列截断\n",
    "    if len(tokens) >= max_seq_length - 1:\n",
    "        tokens = tokens[0:(max_seq_length - 2)]  # -2 的原因是因为序列需要加一个句首和句尾标志\n",
    "        labels = labels[0:(max_seq_length - 2)]\n",
    "    ntokens = []\n",
    "    segment_ids = []  #segment_ids的作用是？？？\n",
    "    label_ids = []\n",
    "    ntokens.append(\"[CLS]\")  # 句子开始设置CLS 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[CLS]\") not sure!\n",
    "    label_ids.append(label_map[\"[CLS]\"])  # O OR CLS 没有任何影响，不过我觉得O 会减少标签个数,不过句首和句尾使用不同的标志来标注，使用LCS 也没毛病\n",
    "    for i, token in enumerate(tokens):\n",
    "        ntokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(label_map[labels[i]])\n",
    "    ntokens.append(\"[SEP]\")  # 句尾添加[SEP] 标志\n",
    "    segment_ids.append(0)\n",
    "    # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "    label_ids.append(label_map[\"[SEP]\"])\n",
    "    #调用bert内部的token2id函数，把token转化成bert使用的tokenid\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(ntokens)  # 将序列中的字(ntokens)转化为ID形式\n",
    "    input_mask = [1] * len(input_ids)   #mask是隐藏的token，用于模型训练\n",
    "    # label_mask = [1] * len(input_ids)\n",
    "    # padding, 使用\n",
    "    #小于序列长度的，进行补全操作\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        # we don't concerned about it!\n",
    "        label_ids.append(0)\n",
    "        ntokens.append(\"**NULL**\")\n",
    "        # label_mask.append(0)\n",
    "    # print(len(input_ids))\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "    # assert len(label_mask) == max_seq_length\n",
    "\n",
    "    # 打印部分样本数据信息\n",
    "    if ex_index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "            [tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "        # tf.logging.info(\"label_mask: %s\" % \" \".join([str(x) for x in label_mask]))\n",
    "\n",
    "    # 结构化为一个类，使用自定义类保存训练index化数据，\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_ids=label_ids,\n",
    "        # label_mask = label_mask\n",
    "    )\n",
    "    # mode='test'的时候才有效，把token写入文件\n",
    "    write_tokens(ntokens, output_dir, mode)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokens(tokens, output_dir, mode):\n",
    "    \"\"\"\n",
    "    将序列解析结果写入到文件中\n",
    "    只在mode=test的时候启用\n",
    "    :param tokens:\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if mode == \"test\":\n",
    "        path = os.path.join(output_dir, \"token_\" + mode + \".txt\")\n",
    "        wf = codecs.open(path, 'a', encoding='utf-8')\n",
    "        for token in tokens:\n",
    "            if token != \"**NULL**\":\n",
    "                wf.write(token + '\\n')\n",
    "        wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filed_based_convert_examples_to_features(\n",
    "        examples, label_list, max_seq_length, tokenizer, output_file, output_dir, mode=None):\n",
    "    \"\"\"\n",
    "    将数据转化为TF_Record 结构，作为模型数据输入\n",
    "    :param examples:  样本\n",
    "    :param label_list:标签list\n",
    "    :param max_seq_length: 预先设定的最大序列长度\n",
    "    :param tokenizer: tokenizer 对象\n",
    "    :param output_file: tf.record 输出路径\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    # 遍历训练数据\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "        # 对于每一个训练样本,将一个样本进行分析，然后将字转化为id, 标签转化为id,然后结构化到InputFeatures对象中\n",
    "        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()#除了记录k,v，还会记录k放入的顺序\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature(feature.label_ids)\n",
    "        # features[\"label_mask\"] = create_int_feature(feature.label_mask)\n",
    "        # tf.train.Example/Feature 是一种协议，方便序列化？？？\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 43\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-0\n",
      "INFO:tensorflow:tokens: 海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。\n",
      "INFO:tensorflow:input_ids: 101 3862 7157 3683 6612 1765 4157 1762 1336 7305 680 7032 7305 722 7313 4638 3862 1818 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 4 1 6 4 1 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-1\n",
      "INFO:tensorflow:tokens: 这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ， 整 个 建 筑 群 精 美 而 恢 宏 。\n",
      "INFO:tensorflow:input_ids: 101 6821 2429 898 2255 988 3717 4638 1300 4289 7667 4507 1744 1079 671 3837 4638 6392 6369 2360 712 2898 6392 6369 8024 3146 702 2456 5029 5408 5125 5401 5445 2612 2131 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-2\n",
      "INFO:tensorflow:tokens: 但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 正 做 到 [UNK] 先 天 下 之 忧 而 忧 ， 后 天 下 之 乐 而 乐 [UNK] ， 淡 化 个 人 的 名 利 得 失 和 宠 辱 悲 喜 ， 把 改 革 大 业 摆 在 首 位 ， 这 样 才 能 超 越 自 我 ， 摆 脱 世 俗 ， 有 所 作 为 。\n",
      "INFO:tensorflow:input_ids: 101 852 868 711 671 702 1066 772 1054 1447 510 782 3696 1062 789 8024 2418 2496 5541 2577 2160 7333 8024 4696 3633 976 1168 100 1044 1921 678 722 2569 5445 2569 8024 1400 1921 678 722 727 5445 727 100 8024 3909 1265 702 782 4638 1399 1164 2533 1927 1469 2143 6802 2650 1599 8024 2828 3121 7484 1920 689 3030 1762 7674 855 8024 6821 3416 2798 5543 6631 6632 5632 2769 8024 3030 5564 686 921 8024 3300 2792 868 711 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-3\n",
      "INFO:tensorflow:tokens: 在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 系 的 重 要 组 成 部 分 。\n",
      "INFO:tensorflow:input_ids: 101 1762 1355 6809 1744 2157 8024 2593 3131 924 7372 1282 1146 3249 1350 8024 2347 2768 711 4852 833 924 7397 860 5143 4638 7028 6206 5299 2768 6956 1146 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-4\n",
      "INFO:tensorflow:tokens: 日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 历 史 最 佳 时 期 ， 但 其 脆 弱 性 不 言 自 明 。\n",
      "INFO:tensorflow:input_ids: 101 3189 915 697 1744 1744 1079 3124 2229 6963 1041 4007 1359 3144 8024 2226 5052 3189 915 1068 5143 4680 1184 3221 1325 1380 3297 881 3198 3309 8024 852 1071 5546 2483 2595 679 6241 5632 3209 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "filed_based_convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer, train_file, args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        # \"label_ids\":tf.VarLenFeature(tf.int64),\n",
    "        # \"label_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        #把recod变成字典example？,可能写入，读取都是按照tensorflow的某个标准\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "    def input_fn(params):\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        d = tf.data.TFRecordDataset(input_file)\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=300)\n",
    "        #通过map函数，调用_decode_record，把int64的数据转化成int32的数据，通过apply，把数据转化成batch的形式\n",
    "        d = d.apply(tf.data.experimental.map_and_batch(lambda record: _decode_record(record, name_to_features),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       num_parallel_calls=8,  # 并行处理数据的CPU核心数量，不要大于你机器的核心数\n",
    "                                                       drop_remainder=drop_remainder))\n",
    "        d = d.prefetch(buffer_size=4)\n",
    "        return d\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = file_based_input_fn_builder(\n",
    "            input_file=train_file,\n",
    "            seq_length=args.max_seq_length,\n",
    "            is_training=True,\n",
    "            drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function file_based_input_fn_builder.<locals>.input_fn at 0x00000181EC309A60>\n"
     ]
    }
   ],
   "source": [
    "print(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 43\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-0\n",
      "INFO:tensorflow:tokens: 海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。\n",
      "INFO:tensorflow:input_ids: 101 3862 7157 3683 6612 1765 4157 1762 1336 7305 680 7032 7305 722 7313 4638 3862 1818 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 4 1 6 4 1 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-1\n",
      "INFO:tensorflow:tokens: 这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ， 整 个 建 筑 群 精 美 而 恢 宏 。\n",
      "INFO:tensorflow:input_ids: 101 6821 2429 898 2255 988 3717 4638 1300 4289 7667 4507 1744 1079 671 3837 4638 6392 6369 2360 712 2898 6392 6369 8024 3146 702 2456 5029 5408 5125 5401 5445 2612 2131 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-2\n",
      "INFO:tensorflow:tokens: 但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 正 做 到 [UNK] 先 天 下 之 忧 而 忧 ， 后 天 下 之 乐 而 乐 [UNK] ， 淡 化 个 人 的 名 利 得 失 和 宠 辱 悲 喜 ， 把 改 革 大 业 摆 在 首 位 ， 这 样 才 能 超 越 自 我 ， 摆 脱 世 俗 ， 有 所 作 为 。\n",
      "INFO:tensorflow:input_ids: 101 852 868 711 671 702 1066 772 1054 1447 510 782 3696 1062 789 8024 2418 2496 5541 2577 2160 7333 8024 4696 3633 976 1168 100 1044 1921 678 722 2569 5445 2569 8024 1400 1921 678 722 727 5445 727 100 8024 3909 1265 702 782 4638 1399 1164 2533 1927 1469 2143 6802 2650 1599 8024 2828 3121 7484 1920 689 3030 1762 7674 855 8024 6821 3416 2798 5543 6631 6632 5632 2769 8024 3030 5564 686 921 8024 3300 2792 868 711 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-3\n",
      "INFO:tensorflow:tokens: 在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 系 的 重 要 组 成 部 分 。\n",
      "INFO:tensorflow:input_ids: 101 1762 1355 6809 1744 2157 8024 2593 3131 924 7372 1282 1146 3249 1350 8024 2347 2768 711 4852 833 924 7397 860 5143 4638 7028 6206 5299 2768 6956 1146 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: dev-4\n",
      "INFO:tensorflow:tokens: 日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 历 史 最 佳 时 期 ， 但 其 脆 弱 性 不 言 自 明 。\n",
      "INFO:tensorflow:input_ids: 101 3189 915 697 1744 1744 1079 3124 2229 6963 1041 4007 1359 3144 8024 2226 5052 3189 915 1068 5143 4680 1184 3221 1325 1380 3297 881 3198 3309 8024 852 1071 5546 2483 2595 679 6241 5632 3209 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label_ids: 5 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "eval_file = os.path.join(args.output_dir, \"eval.tf_record\")\n",
    "if not os.path.exists(eval_file):\n",
    "    filed_based_convert_examples_to_features(\n",
    "        eval_examples, label_list, args.max_seq_length, tokenizer, eval_file, args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = file_based_input_fn_builder(\n",
    "            input_file=eval_file,\n",
    "            seq_length=args.max_seq_length,\n",
    "            is_training=False,\n",
    "            drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_hook = tf.contrib.estimator.stop_if_no_decrease_hook(\n",
    "            estimator=estimator,\n",
    "            metric_name='loss',\n",
    "            max_steps_without_decrease=num_train_steps,\n",
    "            eval_dir=None,\n",
    "            min_steps=0,\n",
    "            run_every_secs=None,\n",
    "            run_every_steps=args.save_checkpoints_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hook = tf.train.SummarySaverHook(\n",
    "            save_secs=1,\n",
    "            output_dir='./log',\n",
    "            scaffold=tf.train.Scaffold(summary_op=tf.summary.merge_all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-23-c63e47610f50>:17: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:*** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (64, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (64, 128)\n",
      "INFO:tensorflow:  name = label_ids, shape = (64, 128)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (64, 128)\n",
      "shape of input_ids (64, 128)\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\bert_base\\bert\\modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\bert_base\\bert\\modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\training\\learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:/project/python_project/bert-lstm-crf-ner/output\\model.ckpt.\n",
      "INFO:tensorflow:loss = 108.23226, step = 1\n",
      "INFO:tensorflow:global_steps = 1, loss = 108.23226\n",
      "INFO:tensorflow:Saving checkpoints for 2 into D:/project/python_project/bert-lstm-crf-ner/output\\model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:*** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
      "INFO:tensorflow:  name = label_ids, shape = (?, 128)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
      "shape of input_ids (?, 128)\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-04T09:08:27Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from D:/project/python_project/bert-lstm-crf-ner/output\\model.ckpt-2\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-04-09:08:44\n",
      "INFO:tensorflow:Saving dict for global step 2: eval_loss = 4.3700943, global_step = 2, loss = 87.66277\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2: D:/project/python_project/bert-lstm-crf-ner/output\\model.ckpt-2\n",
      "INFO:tensorflow:Loss for final step: 99.0119.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'eval_loss': 4.3700943, 'loss': 87.66277, 'global_step': 2}, [])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=num_train_steps,\n",
    "                                            hooks=[early_stopping_hook])\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
